<!DOCTYPE HTML>
<html lang="en" class="light" dir="">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Foundation Model for Climate Notes</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="./mdbook-admonish.css">
        <link rel="stylesheet" href="./theme/catppuccin.css">
        <link rel="stylesheet" href="./theme/catppuccin-admonish.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">IPCC Reports</li><li class="chapter-item expanded "><a href="ipcc_wg1.html"><strong aria-hidden="true">2.</strong> WG1</a></li><li class="chapter-item expanded "><a href="ipcc_wg2.html"><strong aria-hidden="true">3.</strong> WG2</a></li><li class="chapter-item expanded "><a href="ipcc_wg3.html"><strong aria-hidden="true">4.</strong> WG3</a></li><li class="chapter-item expanded "><a href="ipcc_ar6.html"><strong aria-hidden="true">5.</strong> AR6</a></li><li class="chapter-item expanded affix "><li class="part-title">Models</li><li class="chapter-item expanded "><a href="gnns.html"><strong aria-hidden="true">6.</strong> GNNs</a></li><li class="chapter-item expanded "><a href="vits.html"><strong aria-hidden="true">7.</strong> ViTs</a></li><li class="chapter-item expanded "><a href="general_circulation_models.html"><strong aria-hidden="true">8.</strong> General Circulation Models</a></li><li class="chapter-item expanded "><a href="linear_transformers.html"><strong aria-hidden="true">9.</strong> Linear Transformers</a></li><li class="chapter-item expanded "><a href="structured_state_spaces.html"><strong aria-hidden="true">10.</strong> Structured State Spaces</a></li><li class="chapter-item expanded affix "><li class="part-title">Papers</li><li class="chapter-item expanded "><a href="climax.html"><strong aria-hidden="true">11.</strong> ClimaX</a></li><li class="chapter-item expanded "><a href="panguweather.html"><strong aria-hidden="true">12.</strong> PanguWeather</a></li><li class="chapter-item expanded "><a href="fuxi.html"><strong aria-hidden="true">13.</strong> FuXi</a></li><li class="chapter-item expanded "><a href="fourcastnet.html"><strong aria-hidden="true">14.</strong> FourcastNet</a></li><li class="chapter-item expanded "><a href="ace.html"><strong aria-hidden="true">15.</strong> ACE</a></li><li class="chapter-item expanded "><a href="expt.html"><strong aria-hidden="true">16.</strong> ExPT</a></li><li class="chapter-item expanded "><a href="maxvit.html"><strong aria-hidden="true">17.</strong> MaxViT</a></li><li class="chapter-item expanded "><a href="hieravit.html"><strong aria-hidden="true">18.</strong> HieraViT</a></li><li class="chapter-item expanded "><a href="swin_transformer.html"><strong aria-hidden="true">19.</strong> Swin Transformer</a></li><li class="chapter-item expanded "><a href="swin_transformer_v2.html"><strong aria-hidden="true">20.</strong> Swin Transformer V2</a></li><li class="chapter-item expanded "><a href="closed-form_continuous-time_neural_networks.html"><strong aria-hidden="true">21.</strong> Closed-form continuous-time neural networks</a></li><li class="chapter-item expanded "><a href="recurrent_fast_weight_programmers.html"><strong aria-hidden="true">22.</strong> Recurrent Fast Weight Programmers</a></li><li class="chapter-item expanded "><a href="liquid_neural_networks.html"><strong aria-hidden="true">23.</strong> Liquid Neural Networks</a></li><li class="chapter-item expanded "><a href="metnet.html"><strong aria-hidden="true">24.</strong> MetNet</a></li><li class="chapter-item expanded "><a href="spatio-temporal_swin-transformer.html"><strong aria-hidden="true">25.</strong> Spatio-Temporal Swin-Transformer</a></li><li class="chapter-item expanded "><a href="graphcast.html"><strong aria-hidden="true">26.</strong> GraphCast</a></li><li class="chapter-item expanded "><a href="linear_transformers_as_fwp.html"><strong aria-hidden="true">27.</strong> Linear Transformers as FWP</a></li><li class="chapter-item expanded "><a href="ai_cca.html"><strong aria-hidden="true">28.</strong> AI CCA</a></li><li class="chapter-item expanded "><a href="liquid_s4.html"><strong aria-hidden="true">29.</strong> Liquid S4</a></li><li class="chapter-item expanded affix "><li class="part-title">Climate Benchmarks</li><li class="chapter-item expanded "><a href="climatelearn.html"><strong aria-hidden="true">30.</strong> ClimateLearn</a></li><li class="chapter-item expanded "><a href="climatebench.html"><strong aria-hidden="true">31.</strong> ClimateBench</a></li><li class="chapter-item expanded "><a href="weatherbench.html"><strong aria-hidden="true">32.</strong> WeatherBench</a></li><li class="chapter-item expanded affix "><li class="part-title">Scaling Benchmarks</li><li class="chapter-item expanded "><a href="training_llms_supercomp.html"><strong aria-hidden="true">33.</strong> Training LLMs on leadership‐class supercomputers</a></li><li class="chapter-item expanded "><a href="scaling_laws_llms.html"><strong aria-hidden="true">34.</strong> Scaling Laws for LLMS</a></li><li class="chapter-item expanded "><a href="compute_optimal_llms.html"><strong aria-hidden="true">35.</strong> Compute Optimal LLMs</a></li><li class="chapter-item expanded "><a href="data_optimal_llms.html"><strong aria-hidden="true">36.</strong> Data Optimal LLMs</a></li><li class="chapter-item expanded affix "><li class="part-title">Development</li><li class="chapter-item expanded "><a href="parallelization.html"><strong aria-hidden="true">37.</strong> Parallelization</a></li><li class="chapter-item expanded affix "><li class="part-title">Useful Stuff</li><li class="chapter-item expanded "><a href="notes_for_talks.html"><strong aria-hidden="true">38.</strong> Notes for Talks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="a_data_perspective.html"><strong aria-hidden="true">38.1.</strong> A Data-Oriented Perspective</a></li><li class="chapter-item expanded "><a href="ai4good.html"><strong aria-hidden="true">38.2.</strong> Ai for Good talk</a></li></ol></li><li class="chapter-item expanded "><a href="tropical_cyclones.html"><strong aria-hidden="true">39.</strong> Tropical Cyclones</a></li><li class="chapter-item expanded "><a href="model_comparison.html"><strong aria-hidden="true">40.</strong> Model Comparison</a></li><li class="chapter-item expanded "><a href="challenges_and_opportunities.html"><strong aria-hidden="true">41.</strong> Challenges and Opportunities</a></li><li class="chapter-item expanded "><a href="improvements.html"><strong aria-hidden="true">42.</strong> FM4C Improvements</a></li><li class="chapter-item expanded affix "><li class="part-title">Footnotes</li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Foundation Model for Climate Notes</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/lelepado01/FoundationModel4ClimateNotes/tree/main/" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<h3 id="resources"><a class="header" href="#resources">Resources</a></h3>
<blockquote>
<p><a href="https://github.com/shengchaochen82/Awesome-Foundation-Models-for-Weather-and-Climate">Summary</a></p>
</blockquote>
<h3 id="ideas"><a class="header" href="#ideas">Ideas</a></h3>
<p>Agile approach to research, with many small projects (POC), check up every 3-6 months, which can be integrated into larger projects. All with a general topic, one of the following:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Ensamble of models which output the certainty of their prediction, instead of the prediction</li>
<li><input disabled="" type="checkbox"/>
Tool for long range prediction (years) with CMIP data</li>
<li><input disabled="" type="checkbox"/>
Tool for socio-economic decision making with climate data (IPCC report)
<ul>
<li>Multi-modal forecaster using both climate and socio-economic data (ex. average income...)</li>
</ul>
</li>
<li><input disabled="" type="checkbox"/>
Graph compression with GNNs, which can be used for data elaboration</li>
</ul>
<h3 id="useful-datasets"><a class="header" href="#useful-datasets">Useful Datasets</a></h3>
<ul>
<li>World Input-Output Database (WIOD)</li>
<li>Shared Socioeconomic Pathways (SSP)</li>
<li>World Bank Open Data</li>
<li>United Nations Development Programme (UNDP)</li>
<li>Global Burden of Disease (GBD)</li>
<li>Internal monetary fund (IMF)</li>
</ul>
<h3 id="todos"><a class="header" href="#todos">TODOs</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
Paper: The rise of data driven weather forecasts</li>
<li><input disabled="" type="checkbox" checked=""/>
Look at foundation models scaling laws</li>
<li><input disabled="" type="checkbox"/>
Is it possible to finetune for prediction of unseen variables?</li>
<li><input disabled="" type="checkbox"/>
Look into surrogate models, which can be intergrated into larger projects together</li>
<li><input disabled="" type="checkbox" checked=""/>
Project: AI CCA cloud classification atlas </li>
<li><input disabled="" type="checkbox"/>
Look into Temporal GNNs</li>
<li><input disabled="" type="checkbox" checked=""/>
Look into GNNs for climate</li>
<li><input disabled="" type="checkbox" checked=""/>
Look at Neural GCM @ google</li>
<li><input disabled="" type="checkbox" checked=""/>
Look at Neural General Circulation Models</li>
<li><input disabled="" type="checkbox" checked=""/>
Look at Liquid neural networks</li>
<li><input disabled="" type="checkbox" checked=""/>
Look at Recurrent Fast Weight Programmers</li>
<li><input disabled="" type="checkbox"/>
LLaMA (2023) Open and Efficient Foundation Language Models</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ipcc-working-group-1"><a class="header" href="#ipcc-working-group-1">IPCC Working Group 1</a></h1>
<h2 id="the-current-state-of-the-climate"><a class="header" href="#the-current-state-of-the-climate">The Current State of the Climate</a></h2>
<p>It is unequivocal that human influence has warmed the atmosphere, ocean and land. Widespread and rapid changes in the atmosphere, ocean, cryosphere and biosphere have occurred.</p>
<p>Global mean sea level increased by 0.20 [0.15 to 0.25] m between 1901 and 2018. The average rate of sea level rise was 1.3 [0.6 to 2.1] mm yr–1 between 1901 and 1971, increasing to 1.9 [0.8 to 2.9] mm yr–1 between 1971 and 2006, and further increasing to 3.7 [3.2 to 4.2] mm yr–1 between 2006 and 2018 (high confidence). Human influence was very likely the main driver of these increases since at least 1971.</p>
<p><img src="./imgs/ar6_1.png" alt="WG1_1" /></p>
<p>Human-induced climate change is already affecting many weather and climate extremes in every region across the globe. </p>
<p>Improved knowledge of climate processes, paleoclimate evidence and the response of the climate system to increasing radiative forcing gives a best estimate of equilibrium climate sensitivity of 3°C. </p>
<h2 id="possible-climate-futures"><a class="header" href="#possible-climate-futures">Possible Climate Futures</a></h2>
<p>Global surface temperature will continue to increase until at least mid-century under all emissions scenarios considered. Global warming of 1.5°C and 2°C will be exceeded during the 21st century unless deep reductions in CO2 and other greenhouse gas emissions occur in the coming decades.</p>
<p>Many changes in the climate system become larger in direct relation to increasing global warming. They include increases in the frequency and intensity of hot extremes, marine heatwaves, heavy precipitation, and, in some regions, agricultural and ecological droughts; an increase in the proportion of intense tropical cyclones; and reductions in Arctic sea ice, snow cover and permafrost. </p>
<p>It is virtually certain that the land surface will continue to warm more than the ocean surface. It is virtually certain that the Arctic will continue to warm more than global surface temperature, with high confidence above two times the rate of global warming.</p>
<p>Continued global warming is projected to further intensify the global water cycle, including its variability, global monsoon precipitation and the severity of wet and dry events. Also, under scenarios with increasing CO2 emissions, the ocean and land carbon sinks are projected to be less effective at slowing the accumulation of CO2 in the atmosphere.</p>
<p><img src="./imgs/wg1_2.png" alt="WG1_2" /></p>
<p>Many changes due to past and future greenhouse gas emissions are irreversible for centuries to millennia, especially changes in the ocean, ice sheets and global sea level.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ipcc-working-group-2"><a class="header" href="#ipcc-working-group-2">IPCC Working Group 2</a></h1>
<h2 id="observed-and-projected-impacts-and-risks"><a class="header" href="#observed-and-projected-impacts-and-risks">Observed and Projected Impacts and Risks</a></h2>
<p>Human-induced climate change, including more frequent and intense extreme events, has caused widespread adverse impacts and related losses and damages to nature and people, beyond natural climate variability. 
Vulnerability of ecosystems and people to climate change differs substantially among and within regions driven by patterns of intersecting socioeconomic development, unsustainable ocean and land use, inequity, marginalization, historical and ongoing patterns of inequity such as colonialism, and governance. </p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="ipcc_wg2.html#admonition-note"></a></p>
</div>
<div>
<p>Current unsustainable development patterns are increasing exposure of ecosystems and people to climate hazards.</p>
</div>
</div>
<h3 id="risks-in-the-near-term-20212040"><a class="header" href="#risks-in-the-near-term-20212040">Risks in the near term (2021–2040)</a></h3>
<p>Global warming, reaching 1.5°C in the near-term, would cause unavoidable increases in multiple climate hazards and present multiple risks to ecosystems and humans. </p>
<h3 id="mid-to-long-term-risks-20412100"><a class="header" href="#mid-to-long-term-risks-20412100">Mid to Long-term Risks (2041–2100)</a></h3>
<p>Beyond 2040 and depending on the level of global warming, climate change will lead to numerous risks to natural and human systems. 
The magnitude and rate of climate change and associated risks depend strongly on near-term mitigation and adaptation actions, and projected adverse impacts and related losses and damages escalate with every increment of global warming</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ipcc-working-group-3"><a class="header" href="#ipcc-working-group-3">IPCC Working Group 3</a></h1>
<h2 id="recent-developments-and-current-trends"><a class="header" href="#recent-developments-and-current-trends">Recent Developments and Current Trends</a></h2>
<p>Total net anthropogenic GHG emissions6 have continued to rise during the period 2010–2019, as have cumulative net CO2 emissions since 1850. 
Regional contributions to global GHG emissions continue to differ widely. Variations in regional, and national per capita emissions partly reflect different development stages, but they also vary widely at similar income levels. The 10% of households with the highest per capita emissions contribute a disproportionately large share of global household GHG emissions. At least 18 countries have sustained GHG emission reductions for longer than 10 years. </p>
<p><img src="./imgs/ipcc_wg3_1.png" alt="Figure 1.1" /></p>
<p>Globally, the 10% of households with the highest per capita emissions contribute 34–45% of global consumption-based household GHG emissions,21 while the middle 40% contribute 40–53%, and the bottom 50% contribute 13–15%.</p>
<p>The unit costs of several low-emission technologies have fallen continuously since 2010. Innovation policy packages have enabled these cost reductions and supported global adoption. </p>
<p><img src="./imgs/ipcc_wg3_2.png" alt="Figure 1.2" /></p>
<p>There has been a consistent expansion of policies and laws addressing mitigation since AR5. This has led to the avoidance of emissions that would otherwise have occurred and increased investment in low-GHG technologies and infrastructure. Policy coverage of emissions is uneven across sectors.</p>
<p>Global GHG emissions in 2030 associated with the implementation of Nationally Determined Contributions (NDCs) announced prior to COP2623 would make it likely that warming will exceed 1.5°C during the 21st century.24 Likely limiting warming to below 2°C would then rely on a rapid acceleration of mitigation efforts after 2030.</p>
<h2 id="system-transformations-to-limit-global-warming"><a class="header" href="#system-transformations-to-limit-global-warming">System Transformations to Limit Global Warming</a></h2>
<p>Global GHG emissions are projected to peak between 2020 and at the latest before 2025 in global modelled pathways that limit warming to 1.5°C (&gt;50%) with no or limited overshoot and in those that limit warming to 2°C (&gt;67%) and assume immediate action. 
Without a strengthening of policies beyond those that are implemented by the end of 2020, GHG emissions are projected to rise beyond 2025, leading to a median global warming of 3.2°C by 2100. </p>
<p>All global modelled pathways that limit warming to 1.5°C (&gt;50%) with no or limited overshoot, and those that limit warming to 2°C (&gt;67%), involve rapid and deep and in most cases immediate GHG emission reductions in all sectors. </p>
<p>This would involve very low or zero-carbon energy sources, such as renewables or fossil fuels with CCS, demand side measures and improving efficiency, reducing non-CO2 emissions, and deploying carbon dioxide removal (CDR) methods to counterbalance residual GHG emissions.</p>
<p>Reducing GHG emissions across the full energy sector requires major transitions, including a substantial reduction in overall fossil fuel use, the deployment of low-emission energy sources, switching to alternative energy carriers, and energy efficiency and conservation. </p>
<p>Net zero CO2 emissions from the industrial sector are challenging but possible. Reducing industry emissions will entail coordinated action throughout value chains to promote all mitigation options, including demand management, energy and materials efficiency, circular material flows, as well as abatement technologies and transformational changes in production processes.</p>
<p>Urban areas can create opportunities to increase resource efficiency and significantly reduce GHG emissions through the systemic transition of infrastructure and urban form through low-emission development pathways towards net-zero emissions. </p>
<p>In modelled global scenarios, existing buildings, if retrofitted, and buildings yet to be built, are projected to approach net zero GHG emissions in 2050 if policy packages, which combine ambitious sufficiency, efficiency, and renewable energy measures, are effectively implemented and barriers to decarbonisation are removed.</p>
<p>The deployment of carbon dioxide removal (CDR) to counterbalance hard-to-abate residual emissions is unavoidable if net zero CO2 or GHG emissions are to be achieved. The scale and timing of deployment will depend on the trajectories of gross emission reductions in different sectors. </p>
<p>Mitigation options costing USD100 tCO2-eq–1 or less could reduce global GHG emissions by at least half the 2019 level by 2030. </p>
<p><img src="./imgs/ipcc_wg3_3.png" alt="Figure 2.1" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ipcc-assessment-report-6"><a class="header" href="#ipcc-assessment-report-6">IPCC Assessment Report 6</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>Assessment Report (AR6) summarises the state of knowledge of climate change, its widespread impacts and risks, and climate change mitigation and adaptation. It integrates the main findings of the Sixth Assessment Report (AR6) based on contributions from the three Working Groups</p>
<h2 id="current-status-and-trends"><a class="header" href="#current-status-and-trends">Current Status and Trends</a></h2>
<p>Human activities, principally through emissions of greenhouse gases, have unequivocally caused global warming, with global surface temperature reaching 1.1°C above 1850-1900 in 2011-2020. 
The likely range of total human-caused global surface temperature increase from 1850–1900 to 2010–20197 is 0.8°C to 1.3°C, with a best estimate of 1.07°C. </p>
<p>Widespread and rapid changes in the atmosphere, ocean, cryosphere and biosphere have occurred. Human-caused climate change is already affecting many weather and climate extremes in every region across the globe. This has led to widespread adverse impacts and related losses and damages to nature and people. Vulnerable communities who have historically contributed the least to current climate change are disproportionately affected. </p>
<p>Approximately 3.3 to 3.6 billion people live in contexts that are highly vulnerable to climate change. Human and ecosystem vulnerability are interdependent. Regions and people with considerable development constraints have high vulnerability to climatic hazards.</p>
<div id="admonition-warning" class="admonition admonish-warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="ipcc_ar6.html#admonition-warning"></a></p>
</div>
<div>
<p>Impacts on some ecosystems are approaching irreversibility such as the impacts of hydrological changes resulting from the retreat of glaciers, or the changes in some mountain and Arctic ecosystems driven by permafrost thaw.</p>
</div>
</div>
<p>Climate change has reduced food security and affected water security, hindering efforts to meet Sustainable Development Goals. 
In all regions increases in extreme heat events have resulted in human mortality and morbidity. The occurrence of climate-related food-borne and water-borne diseases and the incidence of vector-borne diseases have increased. In assessed regions, some mental health challenges are associated with increasing temperatures. </p>
<p><img src="./imgs/ar6_1.png" alt="AR6_1" /></p>
<p>Adaptation planning and implementation has progressed across all sectors and regions, with documented benefits and varying effectiveness. Despite progress, adaptation gaps exist, and will continue to grow at current rates of implementation. </p>
<p>Growing public and political awareness of climate impacts and risks has resulted in at least 170 countries and many cities including adaptation in their climate policies and planning processes. 
Maladaptation especially affects marginalised and vulnerable groups adversely. 
Key barriers to adaptation are limited resources, lack of private sector and citizen engagement, insufficient mobilization of finance (including for research), low climate literacy, lack of political commitment, limited research and/or slow and low uptake of adaptation science, and low sense of urgency. </p>
<p>Policies and laws addressing mitigation have consistently expanded since AR5. Global GHG emissions in 2030 implied by nationally determined contributions (NDCs) announced by October 2021 make it likely that warming will exceed 1.5°C during the 21st century and make it harder to limit warming below 2°C. 
There are gaps between projected emissions from implemented policies and those from NDCs and finance flows fall short of the levels needed to meet climate goals across all sectors and regions. </p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="ipcc_ar6.html#admonition-note"></a></p>
</div>
<div>
<p>From 2010 to 2019 there have been sustained decreases in the unit costs of solar energy (85%), wind energy (55%), and lithium-ion batteries (85%), and large increases in their deployment.</p>
</div>
</div>
<p>Policies implemented by the end of 2020 are projected to result in higher global GHG emissions in 2030 than emissions implied by NDCs, indicating an ‘implementation gap’. </p>
<h2 id="future-climate-change-risks-and-long-term-responses"><a class="header" href="#future-climate-change-risks-and-long-term-responses">Future Climate Change, Risks, and Long-Term Responses</a></h2>
<p>Continued greenhouse gas emissions will lead to increasing global warming, with the best estimate of reaching 1.5°C in the near term in considered scenarios and modelled pathways. Every increment of global warming will intensify multiple and concurrent hazards. </p>
<p>Global warming will continue to increase in the near term (2021–2040) mainly due to increased cumulative CO2 emissions in nearly all considered scenarios and modelled pathways. In the near term, global warming is more likely than not to reach 1.5°C even under the very low GHG emission scenario and likely or very likely to exceed 1.5°C under higher emissions scenarios. </p>
<p>With further warming, every region is projected to increasingly experience concurrent and multiple changes in climatic impact-drivers. Compound heatwaves and droughts are projected to become more frequent, including concurrent events across multiple locations. Projected regional changes include intensification of tropical cyclones and/or extratropical storms, and increases in aridity and fire weather. </p>
<p>With every increment of global warming, regional changes in mean climate and extremes become more widespread and pronounced. Risks and projected adverse impacts and related losses and damages from climate change escalate with every increment of global warming. 
The level of risk will also depend on trends in vulnerability and exposure of humans and ecosystems. Future exposure to climatic hazards is increasing globally due to socio-economic development trends including migration, growing inequality and urbanisation. </p>
<p><img src="./imgs/ar6_2.png" alt="Ar6_2" /></p>
<p>Some future changes are unavoidable and/or irreversible but can be limited by deep, rapid, and sustained global greenhouse gas emissions reduction. The likelihood of abrupt and/or irreversible changes increases with higher global warming levels. 
Deep, rapid, and sustained GHG emissions reductions would limit further sea level rise acceleration and projected long-term sea level rise commitment.</p>
<div id="admonition-warning-1" class="admonition admonish-warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="ipcc_ar6.html#admonition-warning-1"></a></p>
</div>
<div>
<p>As warming levels increase, so do the risks of species extinction or irreversible loss of biodiversity in ecosystems including forests, coral reefs, and in Arctic regions.</p>
</div>
</div>
<p>Adaptation options that are feasible and effective today will become constrained and less effective with increasing global warming. With increasing global warming, losses and damages will increase and additional human and natural systems will reach adaptation limits. </p>
<p>Limiting human-caused global warming requires net zero CO2 emissions. Cumulative carbon emissions until the time of reaching net zero CO2 emissions and the level of greenhouse gas emission reductions this decade largely determine whether warming can be limited to 1.5°C or 2°C. All global modelled pathways that limit warming to 1.5°C (&gt;50%) with no or limited overshoot, and those that limit warming to 2°C (&gt;67%), involve rapid and deep and, in most cases, immediate greenhouse gas emissions reductions in all sectors this decade. </p>
<p><img src="./imgs/ar6_3.png" alt="AR63" /></p>
<p>If warming exceeds a specified level such as 1.5°C, it could gradually be reduced again by achieving and sustaining net negative global CO2 emissions. This would require additional deployment of carbon dioxide removal, compared to pathways without overshoot, leading to greater feasibility and sustainability concerns. Overshoot entails adverse impacts, some irreversible, and additional risks for human and natural systems, all growing with the magnitude and duration of overshoot. </p>
<h2 id="responses-in-the-near-term"><a class="header" href="#responses-in-the-near-term">Responses in the Near Term</a></h2>
<p>There is a rapidly closing window of opportunity to secure a liveable and sustainable future for all. 
increased international cooperation including improved access to adequate financial resources, particularly for vulnerable regions, sectors and groups, and inclusive governance and coordinated policies. </p>
<p>Deep, rapid, and sustained mitigation and accelerated implementation of adaptation actions in this decade would reduce projected losses and damages for humans and ecosystems. Delayed mitigation and adaptation action would lock in high-emissions infrastructure, raise risks of stranded assets and cost-escalation, reduce feasibility, and increase losses and damages. Near-term actions involve high up-front investments and potentially disruptive changes that can be lessened by a range of enabling policies. </p>
<p>These system transitions involve a significant upscaling of a wide portfolio of mitigation and adaptation options. Feasible, effective, and low-cost options for mitigation and adaptation are already available, with differences across systems and regions. </p>
<p>Prioritising equity, climate justice, social justice, inclusion and just transition processes can enable adaptation and ambitious mitigation actions and climate resilient development. Adaptation outcomes are enhanced by increased support to regions and people with the highest vulnerability to climatic hazards. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gnn-transformers"><a class="header" href="#gnn-transformers">GNN Transformers</a></h1>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>A graph is a kind of data structure, which is composed of nodes (objects) and edges (relationships).</p>
<p>Most of the current GNNs are based on the message passing paradigm, which is a generalization of the convolutional neural networks (CNNs) to non-Euclidean domains.</p>
<p>Often GNNs suffer from the following problems:</p>
<ul>
<li><strong>Over-smoothing</strong>: the information from the nodes is averaged out, and the model cannot distinguish between nodes that are far away from each other.</li>
<li><strong>Over-squashing</strong>: due to the exponential computation with the increase in model depth.</li>
</ul>
<h2 id="use-of-gnns-in-transformers"><a class="header" href="#use-of-gnns-in-transformers">Use of GNNs in Transformers</a></h2>
<p>There have been developed three main ways to use GNNs in Transformers:</p>
<ul>
<li>Auxiliary Modules: GNNs are used to inject auxiliary information to improve the performance of the Transformer.</li>
<li>Improve Positional Embedding with Graph: compress the graph structure into positional embedding vectors, and input them to the Vanilla Transformer.</li>
<li>Improve attention matrix from graph: inject graph priors into the attention computation via graph bias terms. </li>
</ul>
<p>These type of modifications tend to yield improved performance both on node level and graph leevel tasks, more efficient training and inference, and better generalization. This being said different group models enjoy different benefits.</p>
<p>Graph building blocks can be used on top of existing attention mechanisms, can be alternated with self-attention layers, or can be used in conjunction (concatenated) with existing transformer blocks. </p>
<h2 id="some-architectures"><a class="header" href="#some-architectures">Some Architectures</a></h2>
<ul>
<li><strong>GraphTrans</strong>: adds a transformer sub-networkon top of a standard GNN layer. It performs as a specialized architecture to learn local representation, while the transformer sub-network learns global representation of pairwise node interactions.</li>
<li><strong>Grover</strong>: uses two GTransformers to learn node and edge representations, respectively. The node representations are then used to update the edge representations, and vice versa. The inputs are first passed to a GNN which is trained to extract vectors as query, key and value. This layer is then followed by a self-attention layer.</li>
<li><strong>GraphiT</strong>: adopts a Graph Convolutional Kernel Network layer to produce a stucture aware embedding of the graph. The output of this layer is then passed to a self-attention layer.</li>
<li><strong>Mesh Graphormer</strong>: stacks Graph residual blocks on a multi-head self-attention layer. The graph residual block is composed of a graph attention layer and a graph feed-forward layer. It improves the local interactions using a graph convolutional layer in each transformer block.</li>
<li><strong>GraphBERT</strong>: uses graph residual terms in each attention layer. It concatenates the graph residual terms with the original attention matrix, and then applies a linear transformation to the concatenated matrix.</li>
</ul>
<h2 id="positional-embeddings"><a class="header" href="#positional-embeddings">Positional Embeddings</a></h2>
<p>It is also possible to compress the grqph structure into positional embedding vectors, and input them to the Vanilla Transformer. Some approaches adopt Laplacian eigenvectors as positional embeddings, while others use svd vector of adjacent matrices as positional embeddings. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vits"><a class="header" href="#vits">ViTs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-circulation-models"><a class="header" href="#general-circulation-models">General Circulation Models</a></h1>
<h2 id="introduction-3"><a class="header" href="#introduction-3">Introduction</a></h2>
<p>General Circulation Models (GCMs) are a class of models which use a combination of numerical solvers and tuned representations for small scale processes. </p>
<h2 id="neural-gcm"><a class="header" href="#neural-gcm">Neural GCM</a></h2>
<blockquote>
<p><a href="https://arxiv.org/abs/2311.07222">Paper</a> |
<a href="">Code</a></p>
</blockquote>
<p>Neural GCM is a GCM which uses a neural network to represent the small scale processes. 
It is competitive with ML models on 10 days forecasts, and competitive with IFS on 15 days forecasts.</p>
<p>Uses a fully differentiable hybrid GCM of the atmosphere, with a model split into two main subcomponents: </p>
<ul>
<li>A <strong>Differentiable Dynamical Core</strong> (DDC) which solves the equations of motion (dynamic equations); </li>
<li>A <strong>Learned Physics module</strong>, which learns to parametrize a set of physical processes (physics equations) with a neural network.</li>
</ul>
<h2 id="end-to-end-training-of-gcms"><a class="header" href="#end-to-end-training-of-gcms">End-to-end training of GCMs</a></h2>
<p>Uses extended backpropagation between the DDC and the Learned Physics module.</p>
<p>Three loss functions: </p>
<ul>
<li>MSE for accuracy:
Takes into account the layer lead time over the forecast horizon.
Double penalty problem: wrong features at long lead times are penalized more than wrong features at short lead times.</li>
<li>Squared Loss: 
Encourages spectrum to match the data. </li>
<li>MSE for bias: 
Batch average mean amplitude of the bias.</li>
</ul>
<p>Trained on three days rollout data. 
Remained stable for year-long simulations.</p>
<h2 id="stochastic-gcm"><a class="header" href="#stochastic-gcm">Stochastic GCM</a></h2>
<p>Introduces randomness to be able to produce ensambles of forecasts.</p>
<p>Loss is <em>CRPS</em> (Continuous Ranked Probability Score) = Mean absolute error + Variance in ensamble spread</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-transformers"><a class="header" href="#linear-transformers">Linear Transformers</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2006.04768">Paper</a> |
<a href="https://github.com/Kyan820815/Linformer">Code</a> |
<a href="https://www.youtube.com/watch?v=-_2AF9Lhweo&amp;list=WL&amp;index=20&amp;t=144s&amp;pp=gAQBiAQB">Explained</a></p>
</blockquote>
<h2 id="introduction-4"><a class="header" href="#introduction-4">Introduction</a></h2>
<p>How to rout information in a sequence of tokens? -&gt; We use query + key matrices</p>
<ul>
<li>Query: what we are looking for (what info we want to extract)</li>
<li>Key: what type of info the node contains (what info we have)</li>
</ul>
<p>Inner product is used to rout (similarity between query and key). 
This is called <em>soft-routing</em> as it is a weighted average of all the keys (where inner product is larger).</p>
<p>Complexity is \( O(n^2) \), where n is the number of tokens (sequence length), embedding size is d. </p>
<p>\( Q \times K = n \times n \) -&gt; where multi-head attention doesn't help, but the n matrix could be simplified into n/heads matrix.</p>
<p>Ex. 4 heads -&gt; 512 / 4 = 128</p>
<p>We can approximate Q into a low rank matrix, and complexity would be reduced to \( O(n) \). </p>
<h2 id="linear-transformer"><a class="header" href="#linear-transformer">Linear Transformer</a></h2>
<p>\( \text{Attention} = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \)</p>
<p>If the term inside the softmax is low rank, then we can reduce computation. </p>
<p>Eigenvalues of Q and K can be used to determine if matrix needs to be high or low rank.
Results show that most of the times 128 is enough.</p>
<p>How to reduce dimensionality?
We can use a random projection P before the self-attention layer. </p>
<p>\( \text{Attention} = \text{softmax}(\frac{Q(EK)^T}{\sqrt{d_k}})FV \)</p>
<p>So we introduce the E and F matrices (fixed, not learned). The term inside the softmax becomes nxk, while FV is kxn. 
The shapes so are correct for matmult. </p>
<h2 id="results"><a class="header" href="#results">Results</a></h2>
<p>With large sequence lengths, the linear transformer keeps inference times <strong>constant</strong>, as it doesn't depend on the sequence length n but also on k.
Complexity is reduced from \( O(n^2) \) to \( O(nk) \).</p>
<p>How to choose k?</p>
<p>\( k = \frac{5\log(n)}{c} \)</p>
<p>So it depends on n still? Complexity is \( O(n\log(n)) \) now.</p>
<p>However we can make it linear:</p>
<p>\( k = min { \theta(9d \log(d)), 5\theta(\log(n)) } \)</p>
<p>In the first case it is linear in d, in the second case it is linear in n.
We can choose the minimum of the two. So it's enough to downproject the matrix to a dimension of about d. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="structured-state-spaces"><a class="header" href="#structured-state-spaces">Structured State Spaces</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2111.00396">Paper</a> | <a href="https://github.com/state-spaces/s4">Code</a></p>
</blockquote>
<h2 id="introduction-5"><a class="header" href="#introduction-5">Introduction</a></h2>
<p>Especially suitable for long and continuous time series (speech, EEG, ...). </p>
<p>Use seq to seq map to map input to a latent space.</p>
<pre><code>&lt;B, L, D&gt; --&gt; &lt;B, L, D&gt;
</code></pre>
<p>Much more efficient than Transformers, both computationally and memory wise. 
Also better at modelling long term dependencies.</p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="structured_state_spaces.html#admonition-note"></a></p>
</div>
<div>
<p>The new model proposed is essentially a new layer.</p>
</div>
</div>
<h2 id="model"><a class="header" href="#model">Model</a></h2>
<p>These are <strong>continuous time models</strong> (CTMs), which also allow for irregular sampling. </p>
<p>State Space Models (SSM) are parameterized by matrices A, B, C, D, and map an input signal \( u(t) \) to
output \( y(t) \)  through a latent state \( x(t) \). Recent theory on continuous-time memorization derives special A matrices that allow SSMs to capture LRDs mathematically and empirically. (Right) SSMs can be computed
either as a recurrence or convolution.</p>
<p>Drawbacks are that are inefficient and prone to vanishing gradients. 
S4 introduces a novel parameterization that efficiently swaps between these representations, allowing it to handle a wide range of tasks, be efficient at both training and inference, and excel at long sequences.</p>
<p>Combine strength of the former models into State space models (SSMs): </p>
<ul>
<li>Seq to seq: discretization of continuous sequence</li>
<li>RNN: hidden memory state</li>
<li>CNN: efficient computation and parallelization ability</li>
</ul>
<p>\( u(t) \leftarrow y(y) \)</p>
<p>where \( u(t) \) is the input and \( y(t) \) is the output. </p>
<p>\( x'(t) = Ax(t) + Bu(t) \)
\( y(t) = Cx(t) + Du(t) \)</p>
<p>where \( x(t) \) is the hidden state space, \( u(t) \) is the input, \( y(t) \) is the output we are trying to predict, \( A \) is the most important matrix called state matrix. 
\( x(t) \) can be a continuous function or a discrete sequence obtained by sampling.</p>
<p>How do we initialize the state matrix so that it handles long term dependencies?</p>
<p>(these matrices are not learned)</p>
<p>The Hippo approach allows memorization of the input (input reconstruction), even after a long time. It works by encoding the data with Legendre polynomials, which are orthogonal and can be computed efficiently. 
It allows to fully describe the input sequence even after long sequences. </p>
<p>The idea is to condition A with low rank correction (computable as Cauchy kernel). 
A can be computed with the Hippo method, continuous time memorization (set of useful matrices).</p>
<ul>
<li>
<p>To discretize: sample from the continuous function \( x(t) \) at discrete time points \( t_i \)
Convert A to discrete time matrix \( \hat{A} \) using the formula: 
\( \hat{A} = (I - \frac{\delta}{2}A)^{-1} (I + \frac{\delta}{2}A) \)</p>
</li>
<li>
<p>To convolution: simply unroll the timesteps into a single dimension and apply a convolutional layer, this allows parallelization</p>
</li>
</ul>
<div id="admonition-important" class="admonition admonish-tip">
<div class="admonition-title">
<p>Important</p>
<p><a class="admonition-anchor-link" href="structured_state_spaces.html#admonition-important"></a></p>
</div>
<div>
<p>With discrete representation, the efficiency is on par with RNNs. This is the reason the frequency domain is used to apply convolutions, which allows for efficient parallelization.</p>
</div>
</div>
<h2 id="results-1"><a class="header" href="#results-1">Results</a></h2>
<p>A general purpose model, which can be applied to continuous, recurrent and convolutional tasks and spaces. 
It is also efficient, both in terms of memory and computation, and performs better than other models (even transformers) on long sequences.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="climax"><a class="header" href="#climax">ClimaX</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2301.10343">Paper</a> | <a href="https://github.com/microsoft/ClimaX">Code</a></p>
</blockquote>
<p>ClimaX is a foundation model designed to be pre-trained on heterogeneous data sources and then
fine-tuned to solve various downstream weather and climate problems. </p>
<p><img src="./imgs/climax1.png" alt="ClimaX Architecture" /></p>
<p>The set of climate and weather
variables is extremely broad, and predictions may be required for regional or even spatially incomplete
data, even at different resolutions. Current CNN-based architectures are not applicable in these
scenarios, as they require the input to be perfectly gridded, contain a fixed set of variables, and have
a fixed spatial resolution. resolution. Transformer-based architectures, on the other hand, offer much
greater flexibility by treating the image-like data as a set of tokens. As a consequence, the backbone
architecture chosen is a <strong>Vision Transformer</strong> to provide greater flexibility. </p>
<p><img src="./imgs/climax2.png" alt="ClimaX Architecture" /></p>
<p>Two significant changes to this model were implemented. The first change involved variable
tokenization, which includes separating each variable into its own channel and tokenizing the input
into a sequence of patches. The second change was variable aggregation, introduced to speed up
computation by reducing the dimensionality of the input data and to aid in distinguishing between
different variables, thereby enhancing attention-based training.
After combining variables, the vision transformer block can produce output tokens that are then
processed through a linear prediction head to recreate the original image. During the pre-training
phase, a latitude-weighted reconstruction error is used to keep into account the location of the current
patch. For fine-tuning, the ClimaX modules can be frozen, allowing for training only on the intended
part of the architecture. In fact, often only the final prediction head and variable coding modules
need retraining. This model has undergone testing for several downstream tasks, including global and
regional forecasting and prediction for unseen climate tasks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="panguweather"><a class="header" href="#panguweather">PanguWeather</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2211.02556">Paper</a> | <a href="https://github.com/198808xc/Pangu-Weather">Code</a></p>
</blockquote>
<p>Pangu Weather is a transformer architecture trained on three dimensional weather variables, as opposed to Climax, where all data was two dimensional. The lead time
is also handled differently, with the model being trained to predict the weather at a certain time in
the future, as opposed to the approach taken in the ClimaX work, where the lead time is passed as
a parameter during the training phase. </p>
<p><img src="./imgs/pangu2.png" alt="PanguWeather Architecture" /></p>
<p>The former approach is more similar to the one used in this
project, where the simplicity of the dataset allows for a more straightforward implementation of the
lead time, sacrificing some flexibility in the process. Finally, the Pangu weather model features some
advanced techniques which separate it from all other competitors, namely the use of two different
resolutions for the encoding of each variable, allowing the model to capture both large scale and small
scale features, and use the attention mechanism to focus on different parts of the input data at the
same time. </p>
<p><img src="./imgs/pangu1.png" alt="PanguWeather Architecture" /></p>
<p>To achieve these two resolution, an encoder-decoder approach is used, where the encoder
is tasked with the downscaling of input variables, and the decoder is tasked with the upscaling of the
output. All transformer blocks are then applied to the output of the encoder, taking as input both
the low and high resolution information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fuxi"><a class="header" href="#fuxi">FuXi</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2306.12873">Paper</a> | <a href="https://github.com/tpys/FuXi">Code</a></p>
</blockquote>
<p>FuXi is an auto-regressive model for weather forecasting. The model is
based on the U-transformer architecture, and is able to recurrently produce a prediction for the next
timestep, given the previous predictions. </p>
<p><img src="../imgs/fuxi1.png" alt="FuXi" /></p>
<p>To generate a 15 days forecast, it is estimated it takes the
model around 60 iterations, with a lead time of 6 hours. The loss utilized is multi-step, meaning it
takes into account several timesteps at once, minimizing the error for each of them. This is in contrast
with the approach taken in this project, where the loss is computed for each timestep individually. The
U-transformer takes as input around 70 variables, for the current timestep, as well as the the preceding
frame. All the variables used for this model are however restricted to two dimensions, ignoring any
height layer. This architecture is a variation of the vanilla transformer model, and as opposed to the
latter, before passing the encoded information to the self attention blocks, it downscales partially the
input.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fourcastnet"><a class="header" href="#fourcastnet">FourcastNet</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2202.11214">Paper</a> | <a href="https://github.com/NVlabs/FourCastNet">Code</a></p>
</blockquote>
<p>FourcastNet is an architecture based on the <em>Adaptive Fourier Neural Operator</em>, which is a neural network model designed for high-resolution inputs, fused with a
vision transformer backbone. The Fourier Neural Operator is a neural network architecture that uses
a Fourier basis to represent the input data, allowing for the efficient computation of convolutions in
the Fourier domain. </p>
<p><img src="../imgs/fourcastnet1.png" alt="FourcastNet" /></p>
<p>The use of this module allows to have a very small footprint in GPU memory,
which is crucial for the training of large models. For instance, the base model used is
around 10Gb in size, while analogue models with similar number of parameters have a size of around
eight times as large.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ace-a-fast-skillful-learned-global-atmospheric-model-for-climate-prediction"><a class="header" href="#ace-a-fast-skillful-learned-global-atmospheric-model-for-climate-prediction">ACE: A fast, skillful learned global atmospheric model for climate prediction</a></h1>
<blockquote>
<p><a href="https://www.climatechange.ai/papers/neurips2023/14">Paper</a> | <a href="https://github.com/ai2cm/ace">Code</a></p>
</blockquote>
<p>ACE is a global atmospheric model that uses a neural network to learn the dynamics of the atmosphere. </p>
<ul>
<li>200M parameters auto-regressive model</li>
<li>100 km resolution global atmospheric model</li>
<li>6 hours temporal resolution, stable for 100 years</li>
</ul>
<h2 id="model-1"><a class="header" href="#model-1">Model</a></h2>
<p>The architecture uses a <strong>Spherical Fourier Neural Transformer</strong> (SFNT) to learn the dynamics of the atmosphere. 
This model uses a spherical harmonic transform of the grid, which is more suitable for the spherical geometry of the Earth. This enables more efficient computation of convolutions on spherical space.</p>
<p>The dataset used is <strong>FV3GFS</strong>, which is a global atmospheric model with 100 km resolution used as the US weather model. </p>
<p>As normalization of samples, residual scaling is used. Where predicting an output always equal to the input for each variable would mean that each variable contributes equally to the loss (indipendently of the scale of variables).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="experimental-transformer"><a class="header" href="#experimental-transformer">Experimental Transformer</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2310.19961">Paper</a> | <a href="https://github.com/tung-nd/ExPT.git">Code</a></p>
</blockquote>
<p>ExPT is a transformer architecture that uses unsupervised learning and in-context pretraining (few-shot training in Transformers). </p>
<ul>
<li><strong>Pretraining</strong>: unlabeled designs from context X (without score). Using synthetic data, so functions that operate on the same domain as the objective function. </li>
<li><strong>Fine-tuning</strong> (Adaptation): the model conditions using a small amount of pairs &lt;design, score&gt;</li>
</ul>
<p>How do we generate the functions? -&gt; there are several ways (ex. Gaussian Mixtures, Gaussian Processes, etc.)
In this case Gaussian Processes are used with RBF kernels.</p>
<h2 id="model-2"><a class="header" href="#model-2">Model</a></h2>
<p>Encoder model is used for pairs \(&lt;\texttt{design}, \texttt{score}&gt;\), in conjunction with y_m, to create an hidden vector h. Token embedding is done with two linear layers, one for the pairs and one for the score. </p>
<p>How do we model conditional probability? -&gt; we use a <strong>Variational Auto-Encoder</strong> to model the conditional probability \( p_{\theta}(x_i | h_i) \).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="maxvit"><a class="header" href="#maxvit">MaxViT</a></h1>
<blockquote>
<p><a href="https://link.springer.com/chapter/10.1007/978-3-031-20053-3_27">Paper</a> | <a href="https://github.com/google-research/maxvit">Code</a></p>
</blockquote>
<p>Makes use of a new scalable attention model (multi-axis attention). </p>
<ul>
<li><strong>Blocked local attention</strong>: attention is only computed within a block of tokens.</li>
<li><strong>Dilated global attention</strong>: attend to some tokens is a sparse way.</li>
</ul>
<p>Allows for linera complexity interactions between local/global tokens. </p>
<p>Normally attention requires quadratic complexity, but this is reduced to linear complexity. Attention is decomposed into local and global attention (by decomposing spatial axis).</p>
<p>Given:</p>
<p>\( x \in R^{H \times W \times C} \)</p>
<ul>
<li>Normal attention flattens the H and W dimensions into a single one. </li>
<li>Block attention separates the token space into \( &lt; \frac{H}{P} \times \frac{W}{P}, P \times P, C &gt; \) non overlapping windows, each of \( P \times P \) size.</li>
<li>Grid attention subdivides the token space into \( &lt; G \times G, \frac{H}{G} \times \frac{W}{G}, C &gt; \), where each grid point is uniformly spread over the token space. </li>
</ul>
<p>Using both block and grid attention, we can compute attention in linear time and have interaction between local and global tokens. 
Normally block attention underperforms on large token spaces, as it is not able to capture long range dependencies.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hiera-vit"><a class="header" href="#hiera-vit">Hiera ViT</a></h1>
<h2 id="introduction-6"><a class="header" href="#introduction-6">Introduction</a></h2>
<p>Hiera ViT is a hierarchical vision transformer that removes additional bulk operations deemed unnecessary. 
Several components can be removed without affecting performance. 
This leads to a more simple and accurate model, which is also faster. </p>
<h2 id="hiera"><a class="header" href="#hiera">Hiera</a></h2>
<p>Uses strong pretext task (with Masked autoencoder) to teach spatial bias. Local attention is used inside the mask units.</p>
<p>The problem when using masked autoencoders is the fact that we hide coarser information and we proceed deeper in the network. To avoid this, sparse masking is used (deletes patches, not overrides them). This also keeps the difference between token (internal feature representation of the model) and masked units (fixed size across layers). </p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="hieravit.html#admonition-note"></a></p>
</div>
<div>
<p>MAEs are used beacuse they are effective teachers for ViTs.</p>
</div>
</div>
<p>The baseline used is MViTv2, which learns a multiscale representation of the image over 4 stages. First it learns low level features (but high spatial resolution), then at each stage trades channel capacity for spatial resolution. </p>
<p>Pooling attention is used mostly to reduce the dimensionality of the input (especially for K and V, while Q is pooled to transition between stages by reducin spatial resolution).</p>
<h2 id="simplifications"><a class="header" href="#simplifications">Simplifications</a></h2>
<h4 id="relative-positional-embedding"><a class="header" href="#relative-positional-embedding">Relative Positional Embedding</a></h4>
<p>This module was added to each attention block to allow the model to learn relative positional information. This is not necessary when training with MAEs, and absolute positional embeddings can be used. </p>
<h4 id="remove-convolutions"><a class="header" href="#remove-convolutions">Remove Convolutions</a></h4>
<p>Convolutions add unnecessary overhead since they provide benefits only when dealing with images. These are replaced with maxpools, reducing the accuracy of the model by 1.0%, however, when removing also maxpools with stride==1, the accuracy is nearly the same as the original one, but 22% faster.</p>
<h4 id="remove-overlap"><a class="header" href="#remove-overlap">Remove Overlap</a></h4>
<p>Maxpools with 3x3 kernel size cause an issue of dimensionality, which is normally fixed with &quot;separate and pad&quot; technique. By avoiding overlaps between maxpools, this stage is unnecessary as kernel size equals to stride. The accuracy stays the same, but the model is 20% faster.</p>
<h4 id="remove-attention-residual"><a class="header" href="#remove-attention-residual">Remove Attention Residual</a></h4>
<p>Attention residual is used as it helps to learn the pooling attention. It is used between Q and the layer output. </p>
<h4 id="mask-unit-attention"><a class="header" href="#mask-unit-attention">Mask Unit Attention</a></h4>
<p>Mask unit attention is used to learn the spatial bias as well as for dimensionality reduction (removing it would slow down the model). 
Instead local attention is used within the unit masks, so that tokens are grouped already once they arrive at the attention block. We can then perform attention within these groups (units). </p>
<h2 id="results-2"><a class="header" href="#results-2">Results</a></h2>
<p>The model is 100% faster than the baseline, while remaining at the same accuracy for images. </p>
<p>For videos, the model is 150% faster, and accuracy is increased by 2.5%.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="swin-transformer"><a class="header" href="#swin-transformer">Swin Transformer</a></h1>
<blockquote>
<p><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper">Paper</a> | <a href="https://github.com/microsoft/Swin-Transformer">Code</a> </p>
</blockquote>
<h2 id="introduction-7"><a class="header" href="#introduction-7">Introduction</a></h2>
<p>Hierarchical Vision Transformer, representation is computed with shifting window. Self attention is limited non overlapping local windows. Allows for cross-window attention. </p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="swin_transformer.html#admonition-note"></a></p>
</div>
<div>
<p>Features linear computational complexity.</p>
</div>
</div>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<p>Patch size is 4x4 and 3 rgb channels. Linear embedding is applied to the patch, and size is constant C. 
Swin attention is then applied and patch merging is done for 2x2 neighboring patches.
Token reduction by 4: </p>
<p>\( \frac{H}{4} \times \frac{W}{4} \rightarrow \frac{H}{8} \times \frac{W}{8} \)</p>
<p>Output dimension is set to 2C.</p>
<h2 id="swin-attention"><a class="header" href="#swin-attention">Swin Attention</a></h2>
<p>The swin transformer block replaces standard self attention with a sliding window attention, linear MLP layerm and GeLU activation function. Before attention layer normalization is applied.</p>
<p>The swin self attention is computed within a local window of size \( M \times M \), which makes it more scalable then normal self attention. </p>
<p>This approach lacks connection cross window, so cross window attention is introduced. This method uses shifted windows partitioning, which alternates between two partitioning configurations. </p>
<ul>
<li>Partition 8x8 feature map into 2x2 windows;</li>
<li>Shift next layer from the previous by displacing the window. </li>
</ul>
<h4 id="how-to-compute-attention-efficiently"><a class="header" href="#how-to-compute-attention-efficiently">How to compute attention efficiently?</a></h4>
<p>Several windows in the same batch can be created with with cycling shifting process. </p>
<h4 id="relative-bias-problem"><a class="header" href="#relative-bias-problem">Relative Bias Problem</a></h4>
<p>Add bias matrix B before softmax computation.</p>
<p>\( Attn(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}} + B) V \)</p>
<p>where B is a learnable matrix of size \( M^2 \times M^2 \), and \( M^2 \) is the number of patches in a window. 
This matrix can also be optimized by approximation, making it smaller with smaller window size: </p>
<p>\( B \in R^{M^2 \times M^2} \rightarrow B ~ \hat{B} \in R^{(2M-1) \times (2M-1)} \)</p>
<h2 id="variations-of-swin-transformer-model"><a class="header" href="#variations-of-swin-transformer-model">Variations of Swin Transformer model</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>C</th><th>Layer sizes</th><th>Parameters</th></tr></thead><tbody>
<tr><td>Swin-B</td><td>96</td><td>&lt;2, 2, 6, 2&gt;</td><td>29M</td></tr>
<tr><td>Swin-T</td><td>96</td><td>&lt;2, 2, 18, 2&gt;</td><td>50M</td></tr>
<tr><td>Swin-S</td><td>128</td><td>&lt;2, 2, 18, 2&gt;</td><td>88M</td></tr>
<tr><td>Swin-L</td><td>192</td><td>&lt;2, 2, 18, 2&gt;</td><td>88M</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="swin-transformer-v2"><a class="header" href="#swin-transformer-v2">Swin Transformer V2</a></h1>
<blockquote>
<p><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html">Paper</a> | <a href="https://github.com/ChristophReich1996/Swin-Transformer-V2">Code</a></p>
</blockquote>
<h2 id="introduction-8"><a class="header" href="#introduction-8">Introduction</a></h2>
<p>Swin Transformer V2 is an improved version of the Swin Transformer, which is a hierarchical vision transformer. It is designed to scale up to higher capacity and resolution. </p>
<p>Some problems with the original Swin Transformer are:</p>
<ul>
<li>tranining instability</li>
<li>resolution gaps</li>
<li>hunger for labeled data</li>
</ul>
<h2 id="architecture-and-solutions"><a class="header" href="#architecture-and-solutions">Architecture and solutions</a></h2>
<p>For the previous problems, the following solutions are proposed:</p>
<h4 id="residual-post-norm-method"><a class="header" href="#residual-post-norm-method">Residual post norm method</a></h4>
<p>The residual post norm method is used to stabilize training. 
It replaces the pre-norm residual connection used in Swin Transformer with a post-norm residual connection.</p>
<p>The output of the residual block is normalized before merging with the main branch (amplitude of the main branch does not accumulate in the residual branch). 
This means the activation amplitudes are much milder, which makes training more stable.</p>
<h4 id="cosine-attention"><a class="header" href="#cosine-attention">Cosine Attention</a></h4>
<p>Scaled cosine attention is used instead of dot product attention. </p>
<p>\( Sim(q_i, k_j) = \frac{cos(q_i, k_j)}{\tau} + B_{ij} \)</p>
<p>where \( B_{ij} \) is the relative position bias matrix between pixels i and j, while \( \tau \) is a learnable scaling factor.</p>
<h4 id="log-spaced-continuous-bias-term"><a class="header" href="#log-spaced-continuous-bias-term">Log-Spaced continuous bias term</a></h4>
<p>The coordinates are log-spaced, which allows for better resolution of the attention map.</p>
<p>\( \hat{\Delta x} = sign(x) log(1 + \Delta x) \)</p>
<p>where \( \hat{\Delta} x \) is the new log spaced coordinate, and \( \Delta x \) is the original coordinate.</p>
<p>The bias term also uses log-spaced continuous relative positions, instead of the parametrized approach. </p>
<h4 id="self-supervised-pre-training-simmm"><a class="header" href="#self-supervised-pre-training-simmm">Self-supervised pre-training (SimMM)</a></h4>
<p>This method avoids the need for many labeled samples. </p>
<h2 id="scaling-up-model-capacity"><a class="header" href="#scaling-up-model-capacity">Scaling up model capacity</a></h2>
<p>The Swin Transformer V2 is scaled up by increasing the number of layers and channels. 
There are two main issues with this approach:</p>
<ul>
<li>Instability when increasing the number of layers: activations at deeper layers increase drammatically, creating a problem of discrepancy between layers of the network; </li>
<li>Degrade of performance when transfering to different resolutions. </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="closed-form-continuous-time-neural-networks"><a class="header" href="#closed-form-continuous-time-neural-networks">Closed-form continuous-time neural networks</a></h1>
<blockquote>
<p><a href="https://www.nature.com/articles/s42256-022-00556-7">Paper</a> | <a href="https://github.com/raminmh/CfC">Code</a></p>
</blockquote>
<h2 id="introduction-9"><a class="header" href="#introduction-9">Introduction</a></h2>
<p>Closed-form continuous-time (CfC) models resolve the bottleneck of liquid networks (requiring a differential equation solver, which lengthens the inference and training time) by approximating the closed-form solution of the differential equation.</p>
<h2 id="approximating-differential-equations"><a class="header" href="#approximating-differential-equations">Approximating Differential Equations</a></h2>
<p>We need to derive an approximate closed-form solution for LTC networks. </p>
<p>\( x(t) = B \dot e−[wτ + f(x,I;θ)]t \dot f(−x, −I; θ) + A \)</p>
<p>The exponential term in the equation drives the system’s first part (exponentially fast) to 0 and the entire hidden state to A. This issue becomes more apparent when there are recurrent connections and causes vanishing gradient factors when trained by gradient descent. </p>
<p>To reduce this effect, we replace the exponential decay term with a reversed sigmoidal nonlinearity. </p>
<p>The bias parameter B is decided to be part of the trainable parameters of the neural network and choose to use a new network instance instead of f (Bias).</p>
<p>We also replace A with another neural network instance, h(. ) to enhance the flexibility of the model. </p>
<p>Instead of learning all three neural network instances f, g and h separately, we have them share the first few layers in the form of a backbone that branches out into these three functions. As a result, the backbone allows our model to learn shared representations. </p>
<p>The time complexity of the algorithm is equivalent to that of <em>discretized recurrent networks</em>, being at least one order of magnitude faster than <em>ODE-based networks</em>.</p>
<h2 id="problems"><a class="header" href="#problems">Problems</a></h2>
<p>CfCs might express vanishing gradient problems. To avoid this, for tasks that require long-term dependences, it is better to use them together with mixed memory networks. </p>
<p>Inferring causality from ODE-based networks might be more straightforward than a closed-form solution. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recurrent-fast-weight-programmers"><a class="header" href="#recurrent-fast-weight-programmers">Recurrent Fast Weight Programmers</a></h1>
<blockquote>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/3f9e3767ef3b10a0de4c256d7ef9805d-Abstract.html">Paper</a> | <a href="https://github.com/IDSIA/recurrent-fwp">Code</a></p>
</blockquote>
<h2 id="introduction-10"><a class="header" href="#introduction-10">Introduction</a></h2>
<p>With linear transformers, we can prove FWP are effective and fast. 
In this case both slow and fast networks are Feed Forward Networks, and consist of the same layer. </p>
<p>What if we add recurrence to slow and fast networks?</p>
<h2 id="delta-rnn"><a class="header" href="#delta-rnn">Delta RNN</a></h2>
<p>Use RNN for fast weights. </p>
<p>Add recurrent term to feedforward of linear transformer. </p>
<p>\( y^{(t)} = W^{(t)}q^{(t)} + R^{(t)}f(y^{(t-1)}) \)</p>
<p>where \( R^{(t)} \) is the recurrent matrix with additional fast weights, and 
\( f(y^{(t-1)}) \) is the output of fast net at previous time step + softmax activation function. </p>
<h2 id="recurrent-delta-rnn"><a class="header" href="#recurrent-delta-rnn">Recurrent Delta RNN</a></h2>
<p>Make also slow network recurrent via the fast network, taking fast network previous output as input.</p>
<p>\( k^t = W_kx^t + R_k f(y^{t-1}) \)
\( v^t = W_vx^t + R_v f(y^{t-1}) \)
\( q^t = W_qx^t + R_q f(y^{t-1}) \)</p>
<p>where \( R_k, R_v, R_q \) are the recurrent matrices for the slow network (slow weights), and each equation corresponds to query, key and value pairs calculations for attention of the slow network.</p>
<h2 id="problems-1"><a class="header" href="#problems-1">Problems</a></h2>
<p>The network causes additional complexity with regards to linear transformers, but it is still linear (same order of complexity).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="liquid-neural-networks"><a class="header" href="#liquid-neural-networks">Liquid Neural Networks</a></h1>
<blockquote>
<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16936">Paper</a> | <a href="https://github.com/mlech26l/ncps">Code</a> |
<a href="https://www.youtube.com/watch?v=IlliqYiRhMU&amp;pp=ugMICgJpdBABGAHKBRVsaXF1aWQgbmV1cmFsIG5ldHdvcms%3D">Explained</a> </p>
</blockquote>
<h2 id="introduction-11"><a class="header" href="#introduction-11">Introduction</a></h2>
<p>In classical statistics there is an optimal amount of paramterers for a model, after which performance decreases. This problem is known as overparametrization and is also present in neural networks. The recent developments in transformers and vision transformers have shown that overparametrization can be beneficial for performance. </p>
<p>Benefits include new emergent behaviours, more general learning and better generalization and robustness. This is at the cost of increased computational complexity and memory requirements, as well as lower accuracy on minority samples. </p>
<p>Brain inspired, building blocks are neurons and equations from neuron to neuron. </p>
<h2 id="characteristics"><a class="header" href="#characteristics">Characteristics</a></h2>
<p>Liquid neural networks stay adaptable even after training. 
Good for going out of distribution, so for real world applications (drone navigation, self driving cars).</p>
<p>Neural dynamics are continuous processes, so they can be described by differential equations.</p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="liquid_neural_networks.html#admonition-note"></a></p>
</div>
<div>
<p>Synaptic release is more than just a scalar, and adds non-linearity to the system.</p>
</div>
</div>
<h2 id="liquid-state-machines"><a class="header" href="#liquid-state-machines">Liquid State Machines</a></h2>
<p><strong>Continuous time/depth neural networks</strong> (CTRNNs) are a type of recurrent neural network (RNN) where the nodes (neurons) are described by differential equations.</p>
<p>\( \frac{dx(t)}{dx} = f_{n,k,l}(x(t), I(t), \theta) \)</p>
<p>Where f is the neural network, x is the state of the neuron, I is the input and \(\theta\) are the parameters of the network.</p>
<p>The state of the network is the state of all the neurons in the network.</p>
<p>There is no computation for each time step, the network is updated arbitrairly, unlike RNNs.</p>
<p>\( \frac{dx(t)}{dx} = -\frac{x(t)}{\tau} + f_{n,k,l}(x(t), I(t), \theta) \)</p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>We need a numerical ODE solver, to resolve the differential equations.</p>
<p>The backward pass can either be done with the adjoint sensitivity method (loss + neural ODE solver + adjoint state) or with the backpropagation through time method (classic).
The latter method is considered better as it is not a black box. </p>
<h2 id="liquid-time-constant-networks"><a class="header" href="#liquid-time-constant-networks">Liquid Time-Constant Networks</a></h2>
<p><strong>Leaky integrator neural model</strong></p>
<p>\( \frac{dx(t)}{dt} = -\frac{x(t)}{\tau} + f_{n,k,l}(x(t), I(t), \theta) \)</p>
<p>Uses conductance-based synapses, which are more biologically plausible than the classic synapses.</p>
<p>\( S(t) = f_{n,k,l}(x(t), I(t), \theta)(A - x(t)) \)</p>
<p>\( \frac{dx(t)}{dt} = - [\frac{1}{\tau} + f_{n,k,l}(x(t), I(t), \theta)]x(t) + f_{n,k,l}(x(t), I(t), \theta)A \)</p>
<p>The first term is time-dependent, while the second term the input representation at the current time step.</p>
<p>Activations are changed to differential equations, interactions are given by non-linearity (ex. neural nets). </p>
<p>The network might associate the dynamics of the task with its own behaviour (ex. steering left/right implies camera movement).</p>
<p>The liquid time-constant (LTC) model is based on neurons in the form of differential equations interconnected via sigmoidal synapses.</p>
<p>Because LTCs are ordinary differential equations, their behavior can only be described over time. LTCs are universal approximators and implement causal dynamical models. However, the LTC model has one major disadvantage: to compute their output, we need a numerical differential equation-solver which seriously slows down their training and inference time. </p>
<h2 id="expressivity"><a class="header" href="#expressivity">Expressivity</a></h2>
<p>Using the trajectory length method it is possible to measure the expressivity of a network.
The method consists in projecting the latent space of the network onto a lower dimensional space and measuring the length of the trajectory in the lower dimensional space (ex. 2D). </p>
<p>These networks tend to have a higher expressivity than RNNs, but are bad with long term dependencies. </p>
<p>Differential equations can form causal structures, which is good. </p>
<p>Some limitations include: </p>
<ul>
<li>the complexity of this network is tied to the ODE solver, which use fixed steps. Some solutions include Hypersolvers, closed form solutions and sparse flows.</li>
<li>Vanishing gradients and exploding gradients are still a problem. A possible solution is to use a mixed memory wrapper.</li>
</ul>
<h2 id="neural-circuit-policies"><a class="header" href="#neural-circuit-policies">Neural Circuit Policies</a></h2>
<p><strong>Neural Circuit Policies</strong> are recurrent neural network models inspired by the nervous system of the nematode C. elegans. Compared to standard ML models, NCPs have</p>
<ul>
<li>Neurons that are modeled by an ordinary differential equation</li>
<li>A sparse structured wiring</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metnet-3"><a class="header" href="#metnet-3">MetNet 3</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2003.12140">Paper</a> | <a href="https://github.com/lucidrains/metnet3-pytorch">Code</a></p>
</blockquote>
<ul>
<li>Temporal resolution: 2 minutes</li>
<li>Spatial resolution: 1 km</li>
</ul>
<p>The network is a U-Net in conjunction with a MaxVit architecture</p>
<ul>
<li><strong>Topographical embedding</strong>: automatically embeds time-indipendent variables (4km tokens) for 20 parameters</li>
<li><strong>U-Net</strong>: based on a fully convolutional neural network whose architecture was modified and extended to work with fewer training images and to yield more precise segmentation</li>
<li><strong>MaxVit</strong>: hybrid (CNN + ViT) image classification models.</li>
</ul>
<p>Uses parameter oriented training for lead time (0 - 24 hours). 
Masks out with 25% probability a block of data. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spatio-temporal-swin-transformer"><a class="header" href="#spatio-temporal-swin-transformer">Spatio-Temporal Swin-Transformer</a></h1>
<blockquote>
<p><a href="https://www.researchgate.net/profile/Hasan-Al-Marzouqi/publication/354371186_Spatiotemporal_Swin-Transformer_Network_for_Short_time_weather_forecasting/links/61c449a352bd3c7e05874c43/Spatiotemporal-Swin-Transformer-Network-for-Short-time-weather-forecasting.pdf">Paper</a></p>
</blockquote>
<p>Input to the model is 4D with the addition of the temporal dimension.</p>
<p>The input video is defined to be of size T×H×W×3, tokenization is 2x4x4x3: 
In Video Swin Transformer, we treat each 3D patch of size 2×4×4×3 as a token, while the channel size is not patchified.</p>
<p><img src="./imgs/swintransformer1.png" alt="Spatio-Temporal Swin-Transformer" /></p>
<p>Spatial downsampling is applied to reduce the embedding space.
We used a fully connected layer to scale up the dimension of the incoming data. </p>
<p>The proposed network is tested on the Weather4Cast2021 weather forecasting challenge
data, which requires the prediction of 8 hours ahead future frames (4 per hour) from an hour weather product sequence.</p>
<p>This paper used 3D patch embedding, 3D shifted window multi-head self attention as well as patch merging.
This paper has 2d variables as channel dimension is not patchified.
In my case we'll need to create 4D patch embedding as also height layer has to be partitioned.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="graphcast"><a class="header" href="#graphcast">GraphCast</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2212.12794">Paper</a> | <a href="https://github.com/google-deepmind/graphcast">Code</a></p>
</blockquote>
<p>Graphcast is a graph neural network architecture with an encoder-decoder
configuration. The graph neural network is used to encode unstructured input data into a graph
representation. As opposed to, for instance, convolutional layers where neighbouring information
is encoded in a structured grid, graph layers use message passing between nodes to capture the
relationships between different parts of the input data. This allows for the encoding of different kind
of information, not necessarily restricted to a grid configuration.</p>
<p><img src="../imgs/graphcast2.png" alt="GraphCast" /></p>
<p>One important hyperparamter to be set in this kind of architectures is the number of hops the
messages containing neighbouring information are allowed to travel. This is crucial for the model to
learn from the correct amount of knowledge, and allows for reducing the computational complexity of
the model, as the number of hops is directly related to the time required for the model to train.</p>
<p><img src="../imgs/graphcast1.png" alt="GraphCast" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-transformers-as-fwp"><a class="header" href="#linear-transformers-as-fwp">Linear Transformers as FWP</a></h1>
<blockquote>
<p><a href="http://proceedings.mlr.press/v139/schlag21a.html">Paper</a> | <a href="https://github.com/ischlag/fast-weight-transformers">Code</a></p>
</blockquote>
<h2 id="introduction-12"><a class="header" href="#introduction-12">Introduction</a></h2>
<p>The concept of fast weight programmers (FWP) is introduced in <a href="https://arxiv.org/abs/1610.06258">this paper</a>.</p>
<p>The idea is to use a <strong>slow network</strong> to program by gradient descent the weights of a <strong>fast network</strong>. FWP learn to pmanipulate the content of a finite memory and dynamically interact with it. </p>
<p>Linear transformers have constant memory size and time complexity linear which depends on the sequence length. The time complexity is reduced thanks to the linearization of the self-attention layer and softmax operation.</p>
<h2 id="linear-transformers-as-fwp-1"><a class="header" href="#linear-transformers-as-fwp-1">Linear Transformers as FWP</a></h2>
<p>In normal neural networks, the weights are fixed and the input is manipulated, while the activation is input dependant and can change at inference time. 
The idea of FWP is also have the weights variable and input dependent (synaptic modulation). </p>
<ul>
<li>Context-dependent -&gt; slow weights</li>
<li>Context-independent -&gt; fast weights</li>
</ul>
<p>The process revolves around a slow network which is trained to program the weights of the fast network. This makes the fast weights dependent on 
the spatio-temporal context of the input stream.</p>
<p>Which instructions to use? Outer product: </p>
<p>\( a^{(i)}, b^{(i)} = W_ax^{(i)}, W_bx^{(i)} \)
\( W^{(i)} = \sigma (W^{(i-1) + a^{(i)} \oplus b^{(i)}}) \)
\( y^{(i)} = W^{(i)} x^{(i)} \)</p>
<p>The outer product is \( \oplus \), \sigma is the activation function, W_a and W_b are the trainable slow weights, while W is the fast weight matrix.</p>
<h2 id="linearizing-self-attention"><a class="header" href="#linearizing-self-attention">Linearizing self-attention</a></h2>
<p>Instead of removing the softmax, prior works have introduced techniques for linearizing the softmax.
This improves the computational efficiency of the self-attention layer for long sequences. </p>
<p>An important term is the softmax kernel \( \kappa(k, q) = exp(k \dot q) \), which in linear self-attention is approximated by another kernel \( \kappa'(k, q) = \phi(k)^T \phi(q) \).</p>
<p>Since the embedding space for keys is limited, there is only room for d orthogonal vectors. If the length of the sequence is larger than d, th model might be in a overcapacity regime. In this case the model should dynamically interact with the memory content and determine which association to remember and which one to forget. 
On the other hand, the standard transformer stores associations as immutable pairs, increasing its memory requirements.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aicca-ai-driven-cloud-classification-atlas"><a class="header" href="#aicca-ai-driven-cloud-classification-atlas">AICCA: AI-driven Cloud Classification Atlas</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2209.15096v1">Paper</a> | <a href="https://github.com/RDCEP/cloud">Code</a></p>
</blockquote>
<h2 id="why"><a class="header" href="#why">Why?</a></h2>
<p>Clouds are the cause of the most uncertainty in future climate projections. </p>
<h2 id="model--dataset"><a class="header" href="#model--dataset">Model + Dataset</a></h2>
<p>It has been used a rotation invariant auto-encoder + hierarchical agglomerative clustering to capture the distinctions between cloud textures with just raw multi-spectral imagery. </p>
<p>This was used to create a new cloud dataset (AICCA), consisting of AI generated clouds + labels for each cloud type sampled in 22 years of MODIS data. 
This is a NASA hosted aqua and terra satellite imagery dataset. </p>
<h2 id="training"><a class="header" href="#training">Training</a></h2>
<p>The first phase after obtaining the MODIS dataset is to train the RI autoencoder and then define cloud categories by clustering the compact latent representations produced by the trained autoencoder.
The latent space has to explicitly capture the variety of input textures among ocean clouds and also map to differences in physical properties. </p>
<p>In addition, the use of a specific rotation-invariant loss function allows the model to lean in a way that is agnostic to orientation, for similar morphological clouds and thus places those similar clouds in the same cluster. </p>
<h2 id="assigning-clusters"><a class="header" href="#assigning-clusters">Assigning Clusters</a></h2>
<p>Cloud clusters have to be:</p>
<ul>
<li><strong>physically reasonable</strong></li>
<li>capture information on <strong>spatial distributions</strong></li>
<li><strong>separable</strong> and <strong>rotationally invariant</strong> </li>
<li><strong>stable</strong> (produce similar or identical clusters when different subsets of the data are used)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="liquid-structured-state-space-models"><a class="header" href="#liquid-structured-state-space-models">Liquid Structured State Space Models</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2209.12951">Paper</a> | <a href="https://github.com/raminmh/liquid-s4">Code</a> </p>
</blockquote>
<h2 id="introduction-13"><a class="header" href="#introduction-13">Introduction</a></h2>
<p>Linear state space models have been used succesfully to learn representation of sequential data. </p>
<p>In this approach, the structured state space model (S4) is combined with LTC space model to include the input dependant state-transition module. 
The liquid kernel structure takes into account the similarity between samples in sequences at train and inference time. </p>
<h2 id="continuous-time-state-space-model"><a class="header" href="#continuous-time-state-space-model">Continuous-time state space model</a></h2>
<p>\( \hat{x}(t) = Ax(t) + Bu(t) \)
\( y(t) = Cx(t) + Du(t) \)</p>
<p>where \( u(t) \) is a 1d input signal, \( x(t) \) is the hidden state vector, \( y(t) \) is the output vector, and A, B, C, D are the system parameters.</p>
<p>The previous model can then be discretized using the Euler method:</p>
<p>\( x_k = \hat{A}x_{k-1} + \hat{B}u_{k} \)
\( y_k = Cx_k \)</p>
<p>where \( \Delta t \) is the time step, and D is equal to zero. </p>
<p>And convolution can be applied to speed up the computation:</p>
<p>\( x_0 = \hat{B}u_0, x_1 = \hat{AB}u_0 + \hat{B}u_1 \)
\( y_0 = \hat{CB}u_0, y_1 = \hat{CAB}u_0 + \hat{CB}u_1 \)</p>
<p>The equation can be formulated as a convolutional kernel, which can be solved with a Cauchy kernel computation pipeline. </p>
<p>The liquid linearized version of LTC is: </p>
<p>\( \frac{dx(t)}{dt} = - [ A + B \dot f(x(t), u(t), t, \theta) ] \dot x(t) + (\dots)\)</p>
<p>where the term within square brackets is the LTC kernel. </p>
<h2 id="structured-state-space-model--ltc-integration"><a class="header" href="#structured-state-space-model--ltc-integration">Structured state space model + LTC Integration</a></h2>
<p>We can integrate the LTC kernel into the S4 model by using a coupled bi-linear taylor approximation of the former equations. </p>
<p>\( \hat{x}(t) =[ A + Bu(t)] x(t) + Bu(t) \)
\( y(t) = Cx(t) \)</p>
<p>Then we can discretize the previous equations using the Euler method:</p>
<p>\( x_k = \hat{A} + \hat{B}x_{k-1} + \hat{B}u_{k} \)
\( y_k = \hat{C}x_k \)</p>
<p>where \( \Delta t \) is the time step, and D is equal to zero.</p>
<p>And convolution can be applied to speed up the computation:</p>
<p>(other formulas)</p>
<p>\( Y = \hat{K}u + K_{liquid} u_{correlations} \)</p>
<p>How do we compute the liquid kernel efficiently?</p>
<p>HIPPO matrix + NPLR representation + trainable B_n and P_n vectors. </p>
<p>NPLR = low rank factorization \( \leftarrow \) diagonal + low rank matrices</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="climatelearn"><a class="header" href="#climatelearn">ClimateLearn</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2307.01909">Paper</a> | <a href="https://github.com/aditya-grover/climate-learn">Code</a></p>
</blockquote>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Aims at aiding climate forecasting, downscaling and projections. </p>
<p>It is a pytorch integrated dataset, composed mainly of <strong>CMIP6</strong>, <strong>ERA5</strong> and <strong>PRISM</strong> data.</p>
<ul>
<li>
<p>Forecasting: close to medium range weather and climate prediction</p>
</li>
<li>
<p>Downscaling: Due to large grid sizes, large cells are often used for reducing size of data. However, this leads to loss of information, and to a lower resolution predictions. Downscaling aims at correcting bias amd map results to higher resolution.</p>
<p>\( C \times W \times H \leftarrow C' \times W \times H \)
where \( H &lt; H' \) and \( W &lt; W' \)</p>
</li>
<li>
<p>Projections: Obtaining long range predictions under different conditions (ex. greenhouse gasses emission or atmosphere composition). </p>
</li>
</ul>
<p>The library also includes several baselines, pipelines for end-to-end training and evaluation, and a set of metrics.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="climatebench"><a class="header" href="#climatebench">ClimateBench</a></h1>
<blockquote>
<p><a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002954">Paper</a> | <a href="">Code</a></p>
</blockquote>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>The aim is to simulate shared socio-economic pathways. </p>
<p>It is a benchmarking framework from <strong>CMIP</strong>, <strong>AerChemMIP</strong>, <strong>ScenarioMIP</strong>, and <strong>Detection-AttributionMIP</strong>.
It also contains several ML models and full complexity Earth System Models (ESMs).</p>
<p>Mostly used for long term projections.</p>
<p>Also includes <em>piControl</em> (pre-industrial control, 500 years of points) and <em>historical</em> (historical forcing) simulations, which can be used for contrastive learning to reduce the amount of samples required by ML models, as for projections not enough data is available for deep learning training.</p>
<p>A possible challenge is applying ML and statistical learning to high-dimensional data. To this end Linear Transformers could be used.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="weatherbench"><a class="header" href="#weatherbench">WeatherBench</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2002.00469">Paper</a> | <a href="https://github.com/pangeo-data/WeatherBench">Code</a></p>
</blockquote>
<h3 id="resources-1"><a class="header" href="#resources-1">Resources</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h1 id="evaluation-of-pretraining-large-language-models-on-leadershipclass-supercomputers"><a class="header" href="#evaluation-of-pretraining-large-language-models-on-leadershipclass-supercomputers">Evaluation of pre‐training large language models on leadership‐class supercomputers</a></h1>
<blockquote>
<p><a href="https://link.springer.com/article/10.1007/s11227-023-05479-7">Paper</a></p>
</blockquote>
<h2 id="introduction-14"><a class="header" href="#introduction-14">Introduction</a></h2>
<p>The training of large language models (LLMs) is compute intensive and requires large amounts of data.</p>
<h2 id="scaling-laws-of-transformers"><a class="header" href="#scaling-laws-of-transformers">Scaling Laws of Transformers</a></h2>
<p>The loss of LLMs scales with both the training data and model parameters. Consequently, it scales with the amount of computation.</p>
<p>The total number of floating point operations (FLOPs) is approximately, </p>
<p>\( T_{FLOPS} ∼ 6 \times P \times D\)</p>
<p>where P and D are number of model parameters and tokens, respectively. </p>
<p>compared to the attention and other blocks, the feed forward block typically requires the most computation. 
For each element of a feed-forward weight matrix, there are a total of 6 FLOPs per input token. 
the computation hence scales quadratically with the model size.</p>
<p>For example, training a 175B parameter GPT3 model requires \(3.7×10^{24}\) FLOPs, and it quickly grows to \(1.2×10^{26}\) FLOPs for a 1T parameter GPT-style model.</p>
<h2 id="runtime-and-energy-projection"><a class="header" href="#runtime-and-energy-projection">Runtime and energy projection</a></h2>
<p>the runtime can be straightforwardly predicted via</p>
<p>\( t = T_{FLOPs}∕R_{FLOPs} \)
\( ∼ 120 \times P^2∕R_{FLOPs}, \)</p>
<p>where \(T_{FLOPs}\) and \(R_{FLOPs}\) are the compute operations in FLOPs and training performance in FLOPS</p>
<p><img src="./imgs/eval_llms_com_1.png" alt="Eval" /></p>
<p>The energy consumption can be evaluated by: </p>
<p>\(E = t \times R_{𝚆𝚊𝚝𝚝}\)</p>
<p>where \(R_{𝚆𝚊𝚝𝚝}\) is the averaged power measured from few iterations.</p>
<p>For Summit and Crusher: the peak performance in half precision for the V100 (112 TFLOPS) and MI250X (384 TFLOPS) GPUs and linear scaling up to full system</p>
<h2 id="results-3"><a class="header" href="#results-3">Results</a></h2>
<p>The training performance for FSDP and DeepSpeed-Megatron is 62.1 and 65.1 TFLOPS, respectively. From this baseline, the performance drops 20% and 44% for DeepSpeed-Megatron when using tensor parallelism within a NUMA domain and a node on Summit, respectively. The impact for FSDP is less (30% within a node compared to 44% for DeepSpeed-Megatron) due to less frequent communication and a smaller total message size. </p>
<h4 id="scaling-analysis"><a class="header" href="#scaling-analysis">Scaling analysis</a></h4>
<p>We scale up the LLMs training on both Summit and Crusher: </p>
<p><img src="./imgs/eval_llms_com_2.png" alt="Eval" /></p>
<p>The scaling efficiency is about 97%, signaling Frontier can be a promising platform for training LLMs of these model sizes</p>
<h4 id="energy-consumption"><a class="header" href="#energy-consumption">Energy consumption</a></h4>
<p>To estimate the energy usage, we trace the GPU power in watts during the training for FSDP training of GPT 175B model on Summit and Crusher. 
One batch step takes 359 and 301 s, correspondingly. </p>
<p>The averaged power usage is about 85 and 408 Watts, for Summit and Crusher, respectively, and the corresponding computational efficiency is 0.165 and 0.235 TFLOPS/Watt.</p>
<p>As Crusher system bears the same architecture as the first Exascale system Frontier and their unprecedented mix-precision capability, we believe they are well-suited as the platform for training LLMs at extreme scale.</p>
<div id="admonition-important" class="admonition admonish-tip">
<div class="admonition-title">
<p>Important</p>
<p><a class="admonition-anchor-link" href="training_llms_supercomp.html#admonition-important"></a></p>
</div>
<div>
<p>One caveat to consider in our estimation is that the analysis is based on the current implementations of GPT-NeoX (DeepSpeed-Megatron) and PyTorch (FSDP). It’s important to note that the field is rapidly evolving, with ongoing advancements that can further reduce communication costs.</p>
</div>
</div>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>For theoretical peak and achievable performance, the minimum per-device communication bandwidth needed is 37 and 94 GB/s, respectively.
The current 25 GB/s per-device on Crusher is not sufficient to support linear scaling for training GPT 1T model. </p>
<p>We ignore I/O requirement in our analysis because it can be straightforwardly hidden among computations given the typical global batch size of millions of tokens. </p>
<h2 id="ideas-1"><a class="header" href="#ideas-1">Ideas</a></h2>
<ul>
<li>
<p>Graph with Computation need (FLOPs) and ideal training time (days) assuming peak performance and perfect scaling for optimal training of LLMs on Summit and Frontier, respectively. This assumes perfect scaling --&gt; can we do it with estimated scaling from Frontier?</p>
</li>
<li>
<p>Compare real benchmarks with projections from Yin paper</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scaling-laws-for-large-language-models"><a class="header" href="#scaling-laws-for-large-language-models">Scaling Laws for Large Language Models</a></h1>
<h2 id="introduction-15"><a class="header" href="#introduction-15">Introduction</a></h2>
<p>Analysis of cross entropy loss for LLMs. It scales as power law with regards to model size N, dataset size D, and amount of computation for training C. 
Network depth and width have minimal effect on the scaling. </p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="scaling_laws_llms.html#admonition-note"></a></p>
</div>
<div>
<p>Equations govern the dependence between overfitting and training speed.</p>
</div>
</div>
<h2 id="takeaways"><a class="header" href="#takeaways">Takeaways</a></h2>
<ul>
<li>Performance depends on scale, so number of parameters, amount of data, and amount of computation.</li>
<li>Performance depends weakly on network depth and width. </li>
<li>Smooth parameters: performance has a power law dependence with N, D and C.</li>
<li>Performance is good if \(N \approx D \). Performance penalty depends approximately on the ratio \(N^{0.74}/D\). So by increasing the model size by 8x, we would need 5x the data to maintain performance and avoid penalty. </li>
<li>These networks are more sample efficient compared to smaller networks. </li>
<li>Convergence is inefficient, optimal performance is obtained when stopping shortly of convergence. </li>
<li>Optimal batch size is a power of the loss only. It can be calculated by measuring the gradient noise scale.</li>
</ul>
<h2 id="scaling-laws"><a class="header" href="#scaling-laws">Scaling Laws</a></h2>
<ul>
<li>\( L(N) = (\frac{N_c}{N})^\alpha_N\) is the loss by keeping the number of parameters fixed.
where \(N_c \approx 8.8 \times 10^{13} \) and \(\alpha_N \approx 0.076\). 
If we increase model size then the dataset size has to be increased linearly, according to \(D \approx N^{0.74}\).</li>
<li>\( L(D) = (\frac{D_c}{D})^\alpha_D \) is the loss by keeping the dataset size fixed.</li>
<li>\( L(C_{min}) = (\frac{C^{min}_c}{C^{min}})^{\alpha^{min}_c} \) is the loss by keeping the amount of computation fixed.</li>
</ul>
<p>Each \( \alpha \) parameter indicates the degree of performance improvement with respect to the parameter. </p>
<p><img src="./imgs/scaling_laws_1.png" alt="Scaling Laws" /></p>
<h4 id="critical-batch-size"><a class="header" href="#critical-batch-size">Critical Batch Size</a></h4>
<p>The critical batch size is used for understanding the tradeoff speed-efficiency for data parallel training. 
It also obeys the power law L. </p>
<p>\( B_{crit}(L) = \frac{B_*}{L^{\frac{1}{\alpha_B}}} \)</p>
<p>where \(B_* \approx 2^{8}\) and \(\alpha_B \approx 0.21 \).</p>
<p>Critical Batch size follows the same scaling laws as performance increases. It needs to be increased by a factor of 2 for every 13% decrease in loss. In the same way, it is indipendent of network depth and width and model size.</p>
<p>\( B_{crit}(L) = E_{min} / S_{min} \)</p>
<p>where \(E_{min}\) is the minimum amount of samples needed to be processed and  \(S_{min}\) is the minimum amount of steps needed to reach the L. </p>
<h2 id="early-stopping"><a class="header" href="#early-stopping">Early Stopping</a></h2>
<p>Overfitting is proportional to the correction from ending training at \( S_{stop} \), where \( S_{min} \approx S_{stop} \). Also this needs to be an under-estimation, as test loss decreases slower than training loss.</p>
<p><img src="./imgs/scaling_laws_2.png" alt="Early Stopping" /></p>
<p>\( S_{stop}(N, D) \geq \frac{S_{min}}{[L(N, D) - L(N, \inf)]^{\frac{1}{\alpha_s}}} \)</p>
<p>where \( L(N, \inf) \) is the converged loss after training on infinite data. </p>
<div id="admonition-note-1" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="scaling_laws_llms.html#admonition-note-1"></a></p>
</div>
<div>
<p>The more data we have (dataset size D), the less overfitting we have.</p>
</div>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="training-compute-optimal-large-language-models"><a class="header" href="#training-compute-optimal-large-language-models">Training Compute-Optimal Large Language Models</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2203.15556">Paper</a></p>
</blockquote>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>Optimal model size and number of tokens is fixed with the compute budget. </p>
<div id="admonition-note" class="admonition admonish-note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="compute_optimal_llms.html#admonition-note"></a></p>
</div>
<div>
<p>Current LLMs are undertrained</p>
</div>
</div>
<p>What is found is that model size N and numbe of tokens D should scale equally. </p>
<p>\( 2 \times N \implies 2 \times D \)</p>
<p><img src="./imgs/compute_optimal_llms_1.png" alt="COLLMS" /></p>
<p><img src="./imgs/compute_optimal_llms_2.png" alt="COLLMS" /></p>
<p>Given a fixed FLOPs budget, how should we tradeoff model size N and training tokens D?</p>
<ul>
<li>Fix N and vary D (from 70M to 10B)</li>
<li>Fix D and vary N (create 16 FLOPs curves (isoFLOPs))</li>
<li>Fitting a parametric loss function: model all losses as parametric functions of N and D</li>
</ul>
<p>\( \hat{L}(N, D) = E + \frac{A}{N^{\alpha}} + \frac{B}{N{\beta}}\)</p>
<p>where \(E\) is the entropy of natural text, the second term indicates how a transformer with N parameters still underperforms, and the third term is the finite number of optimization steps.</p>
<p>This function can be optimized with Huber loss, creating M isoFLOPs slices and isoLoss contours.</p>
<p><img src="./imgs/compute_optimal_llms_3.png" alt="COLLMS" /></p>
<h2 id="optimal-model-scaling"><a class="header" href="#optimal-model-scaling">Optimal model scaling?</a></h2>
<p>All three approaches yield the same optimal scaling law: to keep C constant, N and D have to scale in a proportional way.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scaling-data-constrained-language-models"><a class="header" href="#scaling-data-constrained-language-models">Scaling Data-Constrained Language Models</a></h1>
<blockquote>
<p><a href="https://arxiv.org/abs/2305.16264">Paper</a> | <a href="https://github.com/huggingface/datablations">Code</a></p>
</blockquote>
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p>In a data constrained regime [900B, 9B] parameters, with a fixed data and compute budget, after four epochs, the delta in loss is negligible.
This implies a decreasing value in repeated tokens. </p>
<p><img src="./imgs/data_optimal_llms_1.png" alt="Data constrained regime" /></p>
<p>What is the best <strong>allocation</strong> and <strong>return</strong> for the given C resources?</p>
<ul>
<li><strong>Allocation</strong>: how to distribute the compute budget between the model size and the number of training tokens?</li>
<li><strong>Return</strong>: how to measure the gain performance?</li>
</ul>
<p>Why repeating data? Cause there may not be enough, and it is cheap to generate, but also repetition for the model, that's why we have epochs.</p>
<p>Measure of validation loss for isoFLOPs: too many epochs lead to overfitting. This under any D. 
For most D sizes, it seems 7 epochs is the best amount. </p>
<p><img src="./imgs/data_optimal_llms_2.png" alt="Data constrained regime" /></p>
<ul>
<li><strong>Allocation</strong>: optimized by scaling epochs more than other parameters. </li>
<li><strong>Return</strong>: sizeable when repeating data, after 16 epochs, the gain is negligible.</li>
</ul>
<p>Scaling laws still hold when repeating several epochs, although with diminishing returns.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parallelization"><a class="header" href="#parallelization">Parallelization</a></h1>
<h2 id="techniques-for-pytorch"><a class="header" href="#techniques-for-pytorch">Techniques for pytorch</a></h2>
<ul>
<li>DDP: the model is copied on all processes, the dataset is split on all the workers and each model is fed a different batch gradient communication is used to keep the models in sync (also overlap of gradient computation)</li>
<li>RPC: used if training paradigm can’t fit using DDP</li>
<li>Collective Comms: foundation for RPC and DDP, low level APIs</li>
</ul>
<h2 id="paradigms"><a class="header" href="#paradigms">Paradigms</a></h2>
<ul>
<li><strong>Model Parallelism</strong>: each worker focuses on a portion of the model, best for large models.</li>
<li><strong>Data Parallelism</strong>: split train set on each worker, shared weights (DDP)</li>
<li><strong>Parameter server architecture</strong>: central node with parameters, workers update the weights by computing the gradient </li>
<li><strong>All-Reduce Comms</strong>: several workers compute private gradient, then combine with all-reduce operation to share global gradient.</li>
<li><strong>Gradient accumulation</strong>: compute gradient on several minibatches, used if comms overhead is high.</li>
</ul>
<h2 id="distributed-deep-learning"><a class="header" href="#distributed-deep-learning">Distributed deep learning</a></h2>
<p>Distributed training is the process of subdividing the training workload of, for example, a large neural
network across multiple processors. These processors are often referred to as workers, and they are
tasked to compute in parallel to speed up the training process. There are two approaches to parallelism:
data and model. In data parallelism, the full training set is divided between all the workers, where a
copy of the model is also kept. Training is done either synchronously, where all the workers wait for
each other, synchronize the gradients, and only then perform the backward step; or asynchronously,
where a selected worker is tasked with keeping an updated version of the weights, and all the others
can read and write from this worker, often called a ”parameter server”. Using the latter procedure
means that all resources are used at the same time, without any delay. However, it also means that
only one worker at a time is training with the latest version of the weights. In large clusters the
centralized nature of this approach can also create bottlenecks. Model parallelism, on the other hand,
divides the model either horizontally, i.e. node-wise, or vertically, i.e. layer-wise, between several
workers who are allowed to run at the same time. This approach also reduces the footprint of the
model in each worker, making it lighter on the GPU’s memory.</p>
<h3 id="ddp"><a class="header" href="#ddp">DDP</a></h3>
<p>Distributed Data Parallel is a method of data parallelism that enables a program to operate on
numerous machines simultaneously. Applications utilizing DDP generate numerous processes and
initialize one DDP instance for each process. </p>
<h3 id="fsdp"><a class="header" href="#fsdp">FSDP</a></h3>
<p>In some cases, it may not be possible to create a duplicate of the model for every process. In
these instances, Fully Sharded Data Parallel may be utilized, where the optimiser states, gradients,
and parameters of the model are subdivided across all DDP ranks. In this case, the neural network is
divided into smaller sub-models, each of which represents a portion of the parameters of the overall
model. This approach allows different parts of the model to be processed simultaneously by different
processing units, and can be used in conjunction with a data-parallel approach that splits the training
set to achieve even faster processing times. This results in a program that has less impact on GPU
memory, thus reducing execution times.</p>
<h3 id="deepspeed"><a class="header" href="#deepspeed">DeepSpeed</a></h3>
<p><strong>DeepSpeed</strong> is a deep learning optimisation suite that enables efficient scalability and faster
execution times for both training and inference of large machine learning models. It was developed
by Microsoft and claims to offer a 15x speedup over other state-of-the-art parallelization techniques.
It provides memory efficient data parallelism and enables training without model parallelism through
a novel solution called Zero Redundancy Optimizer. Unlike basic data parallelism, where memory
states are replicated across data-parallel processes, ZeRO partitions model states and gradients to save
significant memory. Several other memory optimisation techniques are also used, such as Constant
Buffer Optimisation, which combines all communication-based operations into a single operand, and
Contiguous Memory Optimisation, which reduces fragmentation during training.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="notes-on-talks"><a class="header" href="#notes-on-talks">Notes on Talks</a></h1>
<ul>
<li><a href="./ai4good.html">AI for Good talk</a></li>
<li><a href="./a_data_perspective.html">A Data-Oriented Perspective</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-data-oriented-perspective"><a class="header" href="#a-data-oriented-perspective">A Data-Oriented Perspective</a></h1>
<h2 id="these-are-data-driven-approaches"><a class="header" href="#these-are-data-driven-approaches">These are Data-Driven approaches</a></h2>
<p>Here are the major climate-centered datasets used in the field:</p>
<div class="table-wrapper"><table><thead><tr><th>Dataset</th><th>Size</th><th>Where?</th></tr></thead><tbody>
<tr><td>MACCA</td><td>60 TB</td><td>Nasa</td></tr>
<tr><td>CMIP6</td><td>25 TB</td><td></td></tr>
<tr><td>Earth System Configuration Grid</td><td>25 TB</td><td>ORNL</td></tr>
<tr><td>ERA5</td><td>1.5 PB</td><td>ECMWF</td></tr>
<tr><td>ARM</td><td>50 TB</td><td>ORNL</td></tr>
</tbody></table>
</div>
<p>Information as value chain: how information is created, stored, and used in a particular context.</p>
<ul>
<li>How can we do this?</li>
<li>How are agencies taken from agencies and used in the field?</li>
<li>Is data free of errors?</li>
<li>Without any bias?</li>
<li>Is the data reliable?</li>
</ul>
<h1 id="model-summary"><a class="header" href="#model-summary">Model Summary</a></h1>
<p>Here are some of the state-of-the-art models in the field and their performance:</p>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Precision</th><th>Forecast time</th></tr></thead><tbody>
<tr><td>PanguWeather</td><td>&lt; 50 m</td><td>7 days</td></tr>
<tr><td>GraphWeather</td><td>&lt; 50 m</td><td>7 days</td></tr>
<tr><td>GraphCast</td><td>&lt; 60 m</td><td>8 days</td></tr>
<tr><td>ClimaX</td><td>&lt; 100 m</td><td>7 days</td></tr>
</tbody></table>
</div>
<p>Other approaches: </p>
<ul>
<li>Swin-SpatioTemporal Transformer: currently evaluated for NASA project 
Higher complexity, better for small scale</li>
<li>Spherical Fourier Neural Transformer: to avoid noise and blurring problem
Due to spherical projection of grid, problem is accentuated at the poles. 
This model is able to avoid this problem, using this operator for sphere geometry.</li>
<li>GraphCast is better at small resolution and at different scales. 
Also better at compressing information.</li>
<li>IFS uses global 9 km resolution data. On long term forecasts, it is better than ML models.
However with higher resolution data (25km data), ML models are better.</li>
<li>Ensamble Forecasts: run several simulation from same initial conditions, and average the results.
This is a common technique in NWP. Same approach can be used for ML models.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ai-4-good"><a class="header" href="#ai-4-good">AI 4 Good</a></h1>
<h2 id="second-climate-forecast-revolution"><a class="header" href="#second-climate-forecast-revolution">Second Climate Forecast Revolution</a></h2>
<p>As the first one revolved around Numerical Weather prediction forecasts (solving physics equations to predict weather eg. IFS). </p>
<h3 id="weatherbench-1"><a class="header" href="#weatherbench-1">Weatherbench 1</a></h3>
<p>First attempt at a ML data oriented approach to weather forecasting.
First winter of AI for climate, as not enough data was available. 
Data-based approaches were not precise enough to reach NWP levels of accuracy.</p>
<h3 id="weatherbench-2"><a class="header" href="#weatherbench-2">Weatherbench 2</a></h3>
<ul>
<li>Data: in Zarr format + IFS baselines</li>
<li>Evaluation Code: usind datacloud or other remote computing services (colab, aws, etc)</li>
<li>Evaluation platform: interactive graphs, for user visualization</li>
</ul>
<h2 id="are-ai-models-just-blurring"><a class="header" href="#are-ai-models-just-blurring">Are AI models just blurring?</a></h2>
<p>How do we understand this factor?
First we can check if the model is able to predict extremes (or is just averaging the data).</p>
<p>Blurring exists, but is limited to small scales and does not influence the prediction of extremes.
Many ML models have been used for Hurrican Season prediction. 
Graphcast is better than NWP. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tropical-cyclones"><a class="header" href="#tropical-cyclones">Tropical Cyclones</a></h1>
<h2 id="quick-notes"><a class="header" href="#quick-notes">Quick Notes</a></h2>
<p>Structure: cyclones develop in the presence of these conditions: </p>
<ul>
<li>warm water of tropical oceans (&gt;26.5C) → collects energy through convection (?)</li>
<li>unstable atmosphere, cooling fast enough to cause thunderstorms</li>
<li>moist middle atmosphere (humidity) </li>
<li>low vertical wind shears, and little change in wind direction with change in height</li>
</ul>
<p>Tropical Cyclone Genesis potential index (GPI) → accurate, but only at low resolutions</p>
<p>Often these are good for spatial correlation but bad for temporal one (difficulty in predicting inter-annual events)
Solution is to use evolutionary algorithms to obtain Pareto Front of possible solutions (all possible optimal trade-offs between spatial and temporal optimality) → all solutions still have non acceptable temporal resolution.</p>
<div id="admonition-important" class="admonition admonish-tip">
<div class="admonition-title">
<p>Important</p>
<p><a class="admonition-anchor-link" href="tropical_cyclones.html#admonition-important"></a></p>
</div>
<div>
<p>Tropical storms are very rare, lots of samples where the event returns negative, only a small portion positive.</p>
</div>
</div>
<h2 id="physics"><a class="header" href="#physics">Physics</a></h2>
<p>A tropical cyclone is a storm system that rotates rapidly, featuring a low-pressure center, intense
winds, and an organized series of thunderstorms that cause intense rain and sudden gusts.
The term tropical refers to the geographical origin of these systems, which form almost exclusively
over tropical seas, while cyclone refers to their winds moving in a circle, around a central eye, with
surface winds blowing counterclockwise in the Northern Hemisphere and clockwise in the Southern
one.
These cyclones have a diameter most often found
between 100 and 2,000 km. The powerful swirling winds of a tropical cyclone, as the ones shown
in Figure 2.3, arise due to the Earth’s rotation imparting angular momentum as air moves towards
the axis of rotation. These storms are generally most severe when over or near water and quickly
lose intensity when moving over land. Damage can result from strong winds, rain, high waves, and
storm surges, all of which are phenomena of rising water caused by high-speed winds pushing water
towards the coast.</p>
<p>These tropical storms are low-pressure regions in the troposphere. The pressure is the lowest
near the surface, while at the center of these storms sea level pressures are among the lowest ever
observed. These systems are called ”warm core” because the environment near their center is warmer
than the ambient temperature at all heights.
At the periphery of the storm, the air may be nearly calm; however, because of the Earth’s rotation, the air
possesses non-zero absolute angular momentum. As the air flows radially inwards, it starts rotating
cyclonically so as to conserve angular momentum effectively. At a certain distance from the centre of the storm,
air starts moving upwards towards the top of the troposphere.
The air, once lifted, moves away from the storm’s centre and forms
a layer of high clouds called ”cirrus clouds”. These processes ultimately create a wind field that is
almost symmetrical around the storm’s centre. Wind speeds are low at the centre, increase moving
outwards towards the radius of maximum winds and then decay more gradually with radius.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="comparing-state-of-the-art-models"><a class="header" href="#comparing-state-of-the-art-models">Comparing State-of-the-Art Models</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="challenges-and-opportunities"><a class="header" href="#challenges-and-opportunities">Challenges and Opportunities</a></h1>
<blockquote>
<p><a href="https://arxiv.org/pdf/2312.03014.pdf">Paper</a></p>
</blockquote>
<h2 id="difficulties"><a class="header" href="#difficulties">Difficulties</a></h2>
<ul>
<li>Post-processing of data
<ul>
<li>Costs for specific scenarios and analysis (ex. outliers in rare events)</li>
<li>Under-utilization of existing data since it is expensive to process</li>
</ul>
</li>
<li>Data quality and quantity</li>
</ul>
<h2 id="opportunities"><a class="header" href="#opportunities">Opportunities</a></h2>
<ul>
<li>Multimodal models: radar, satellite, numerical weather prediction, etc.</li>
<li>Interpretable models / explainable AI / causal AI</li>
<li>Generizable models
<ul>
<li>can the model predict out of scope?</li>
<li>can the model avoid bias and flaws in the training data?</li>
</ul>
</li>
<li>Continuous learning: can the model learn from new data?</li>
<li>On-device adaptation: customize a model based on local data (ex. adjust to local climate)</li>
<li>Federated Learning: each company trains their own model, but they can share their models to improve the overall model. Global model learns from updates from local models.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foundation-model-for-climate-improvements"><a class="header" href="#foundation-model-for-climate-improvements">Foundation Model for Climate Improvements</a></h1>
<h2 id="incremental-probability-in-cyclone-prediction"><a class="header" href="#incremental-probability-in-cyclone-prediction">Incremental Probability in Cyclone Prediction</a></h2>
<p>One possible improvement to the presented work arises from the fact that the dataset currently illustrates the likelihood of a cyclone being present in a patch with a simple Gaussian distribution. Nonetheless, as the weather variables increasingly make it more plausible, the probability of a cyclone should raise over time. It may be possible to encode this behaviour by manipulating the standard deviation parameter of the Gaussian, resulting in the probability area progressively expanding over time. This would require determining when a patch is displaying indications of forming a tropical cyclone and identifying the exact central position in the patch.</p>
<h2 id="parameter-oriented-training"><a class="header" href="#parameter-oriented-training">Parameter oriented training</a></h2>
<p>One of the main differences between the current implementation of the model and the one presented in the ClimaX paper, is the fact that the latter is trained to predict the input variables shifted by a certain amount of lead time, and the training is repeated for several lead times. This allows the architecture to learn the correct behaviour of each variable over time, and enhances its flexibility by allowing at inference time to take a lead time parameter, and output the correct prediction. This approach has been referenced to as ”parameter oriented training”, and by its very flexible nature allows for a more general purpose model, which can be used for several different tasks.</p>
<h2 id="global-forecasting-system"><a class="header" href="#global-forecasting-system">Global forecasting system</a></h2>
<p>In a similar way to ClimaX’s approach, it may be feasible to merge the image patches and generate a worldwide weather forecast. This would not necessitate any modifications to the current model since the global image normalization is already executed. Additionally, the dataset can be adjusted to share an N-pixel border with adjoining patches and prevent loss of nearby data.
This modification addresses weather variable forecasting and has no impact on this project’s fine- tuning section. Predicting global cyclones may not be necessary since the regions where they most often form are widely recognized, and a regional forecasting approach would be enough.</p>
<h2 id="time-series-transformers"><a class="header" href="#time-series-transformers">Time-Series Transformers</a></h2>
<p>Among several advantages of the transformer architecture, the ability to capture long-range depen- dencies and interactions is particularly central to time series modeling.
A possible improvement to the current work could revolve around increasing the dimensionality of the input data by adding a temporal dimension, and using this information to better predict the future weather. This would imply passing a sequence of time snapshots to the model, and the tensor’s dimensions would become &lt;B, T, V, H, Lat, Lon&gt;, where T is the number of time snapshots.</p>
<p>As for the modifications necessary to the current architecture, the positional encoding of the transformer has to be changed, to allow for the correct encoding of the temporal dimension. What has been done in similar works, is allow for the embedding to be learned from time series data, and not be fixed as in the current work. A similar approach is to use the timestamp information to influence the training of these embedding layers, and allow for a more accurate representation of the data.</p>
<h2 id="ensemble-of-models"><a class="header" href="#ensemble-of-models">Ensemble of models</a></h2>
<h2 id="connection-to-the-intertwin-project"><a class="header" href="#connection-to-the-intertwin-project">Connection to the Intertwin project</a></h2>
<p>One possible mean of unifying the two works could revolve around the use of graph neural network architectures, and integrating these mostly novel approaches into large foundation models. While Fronza’s work revolved around graph neural networks, there exist in the literature examples of applications of these kind of models to global weather forecasting, which is an essential requirement for developing a foundation model based on GNNs. These techniques have already been tested in the literature, where Graph Transformers are used to generate text, processing both the data with the classical approach, but also building a knowledge graph of the sentence, enhancing the understanding of the context. This approach could be used to build a foundation model which is able to understand the context of the input data, and use this knowledge to improve the prediction ability of the model. In this case, the context could be the current regional weather, and the model could use this information to better predict the future weather</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>